{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load epub book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a RAG (Retrieval-Augmented Generation) question-answering system for \"A Christmas Carol\" by\n",
    "  Charles Dickens. Here's what it does:\n",
    "\n",
    "  1. Document Loading & Chunking (Cells 1-4)\n",
    "\n",
    "  - Loads the EPUB book using UnstructuredEPubLoader\n",
    "  - Splits the book into 203 chunks using RecursiveCharacterTextSplitter\n",
    "    - Chunk size: 1024 characters\n",
    "    - Chunk overlap: 50 characters\n",
    "  - Each chunk contains a portion of the book's text\n",
    "\n",
    "  2. Embedding Creation (Cells 5-8)\n",
    "\n",
    "  - Uses the BAAI/bge-small-en-v1.5 sentence transformer model\n",
    "  - Converts text into 384-dimensional vector embeddings\n",
    "  - These embeddings capture semantic meaning for similarity search\n",
    "\n",
    "  3. Vector Database Setup (Cells 9-13)\n",
    "\n",
    "  - Creates unique IDs for each chunk (203 total)\n",
    "  - Stores all chunks in ChromaDB (in-memory vector database)\n",
    "  - Collection name: carol\n",
    "  - Enables semantic search over the book content\n",
    "\n",
    "  4. Basic Retrieval Test (Cells 14-15)\n",
    "\n",
    "  - Tests querying with: \"What happened Marley?\"\n",
    "  - Returns top 5 most relevant chunks based on semantic similarity\n",
    "  - Shows distances (lower = more similar) ranging from 0.31 to 0.33\n",
    "\n",
    "  5. RAG Question-Answering Workflow (Cells 17-21)\n",
    "\n",
    "  The innovative two-step RAG approach:\n",
    "\n",
    "  Step 1: Question Reformulation (Cell 18)\n",
    "  - Takes original question: \"What is the name of Bob Cratchit's youngest son who is ill?\"\n",
    "  - Uses FLAN-T5 to convert to a declarative statement: \"Bob Cratchit's youngest son is ill.\"\n",
    "  - This helps improve retrieval accuracy\n",
    "\n",
    "  Step 2: Retrieve & Answer (Cells 19-21)\n",
    "  - Queries ChromaDB with the reformulated statement (not the question)\n",
    "  - Retrieves top 3 relevant chunks as context\n",
    "  - Combines context + original question into a prompt: \"Answer based on context:\\n\\n{context}\\n\\n{question}\"\n",
    "  - Uses FLAN-T5 to generate final answer: \"Tiny Tim\"\n",
    "\n",
    "  6. Discussion Points (Cell 22)\n",
    "\n",
    "  The notebook ends with reflection questions about:\n",
    "  - Performance of the solution\n",
    "  - Potential issues\n",
    "  - Possible improvements\n",
    "\n",
    "  ---\n",
    "  Key Innovation: The two-step approach (reformulate question \u2192 retrieve \u2192 answer) helps bridge the gap between how\n",
    "  questions are asked and how information is stored in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a text splitter that breaks large documents into smaller, overlapping chunks. Here's what each part\n",
    "  does:\n",
    "\n",
    "  Parameters\n",
    "\n",
    "  chunk_size = 1024\n",
    "  - Maximum size of each text chunk in characters (not words)\n",
    "  - Each chunk will be \u22641024 characters long\n",
    "  - Think of it as cutting the book into pages of roughly equal size\n",
    "\n",
    "  chunk_overlap = 50\n",
    "  - Number of characters that overlap between consecutive chunks\n",
    "  - The last 50 characters of chunk N are repeated as the first 50 characters of chunk N+1\n",
    "  - Prevents important information from being split across chunk boundaries\n",
    "\n",
    "  Why Overlap Matters\n",
    "\n",
    "  Without overlap:\n",
    "  Chunk 1: \"...and Scrooge saw the ghost of\"\n",
    "  Chunk 2: \"Marley appear before him...\"\n",
    "  \u274c Context is broken - the ghost identity is split!\n",
    "\n",
    "  With 50-char overlap:\n",
    "  Chunk 1: \"...and Scrooge saw the ghost of Marley\"\n",
    "  Chunk 2: \"ghost of Marley appear before him...\"\n",
    "  \u2705 Both chunks maintain context about who the ghost is\n",
    "\n",
    "  RecursiveCharacterTextSplitter\n",
    "\n",
    "  This is a smart splitter from LangChain that:\n",
    "  1. Tries to split on natural boundaries (paragraphs first)\n",
    "  2. Falls back to sentences if paragraphs are too long\n",
    "  3. Falls back to individual characters if needed\n",
    "  4. Respects the chunk_size limit while keeping text coherent\n",
    "\n",
    "  Result: The Christmas Carol book is split into 203 semantically meaningful chunks that can be searched independently\n",
    "  while maintaining context at boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load document \n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "epub_loader = UnstructuredEPubLoader('./docs/charles-dickens_a-christmas-carol.epub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO Split document\n",
    "chunks = epub_loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "page_content='For the people who were shovelling away on the housetops were jovial and full of glee; calling out to one another from the parapets, and now and then exchanging a facetious snowball\ufeff\u2014better-natured missile far than many a wordy jest\ufeff\u2014laughing heartily if it went right, and not less heartily if it went wrong. The poulterers\u2019 shops were still half open, and the fruiterers\u2019 were radiant in their glory. There were great, round, potbellied baskets of chestnuts, shaped like the waistcoats of jolly old gentlemen, lolling at the doors, and tumbling out into the street in their apoplectic opulence: There were ruddy, brown-faced, broad-girthed Spanish onions, shining in the fatness of their growth like Spanish friars, and winking from their shelves in wanton slyness at the girls as they went by, and glanced demurely at the hung-up mistletoe. There were pears and apples clustered high in blooming pyramids; there were bunches of grapes, made, in the shopkeepers\u2019 benevolence, to dangle from conspicuous hooks that' metadata={'source': './docs/charles-dickens_a-christmas-carol.epub'}\n"
     ]
    }
   ],
   "source": [
    "# TODO Examine chunk\n",
    "print(len(chunks))\n",
    "print(chunks[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0349629b830e40f2b31db5714dc0a969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587ca140cbd146f193971a661ae55aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625f61d561414f8a915a66ad8ba971e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65372d9544514aa8b2a946fe8aeac2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1e88f17e2f455db5bfb0535cd93ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f2019f9a6e4a159970e65724aca610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ff18de6c04acc82374d8d4e77a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e6d9da1a0e4e7c8f7212b6fa85e1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373a52fdada46e689f3ceb400c442a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b64cc7fbb84ef8adba3594f1afb688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbc6e069b814e7d88cf766b8d57847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create embedding model\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "#embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "chroma_embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore embedding model\n",
    "text = 'hello world'\n",
    "emb_text = chroma_embed_func([ 'hello, world', 'big black bug bleeds black blood' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "384\n",
      "384\n",
      "[-3.15818861e-02 -4.86476459e-02  3.21323797e-02 -6.57482818e-02\n",
      " -1.12419424e-03  1.14271725e-02 -1.62230618e-03  5.49601130e-02\n",
      "  4.48704585e-02 -2.09966279e-03  7.87420943e-03 -2.20074728e-02\n",
      "  3.43550295e-02  6.57045990e-02  2.98711378e-02 -2.77358951e-04\n",
      "  1.02012476e-03 -3.47684883e-02 -1.21079311e-01 -1.47990584e-02\n",
      "  9.72585976e-02  3.53694521e-02 -1.68968178e-02 -4.28635776e-02\n",
      " -2.48042773e-02  5.63818216e-03  6.80470234e-03  1.35493865e-02\n",
      "  6.07596058e-03 -9.83635336e-02 -6.45544007e-02 -1.15323970e-02\n",
      "  3.96090522e-02  2.41095815e-02  4.54738513e-02 -2.10404806e-02\n",
      "  2.52141189e-02 -1.03885625e-02 -7.94329867e-02  3.64228641e-03\n",
      "  4.60232683e-02 -5.09504527e-02  1.40664354e-02 -3.41338338e-03\n",
      "  1.36136273e-02 -4.93411645e-02  1.70672853e-02  5.47222272e-02\n",
      " -2.78037954e-02  4.88214078e-04 -5.45994267e-02 -8.51237681e-03\n",
      " -1.97877735e-02 -2.24599219e-03  2.84830965e-02  9.09864530e-02\n",
      "  7.97385275e-02  2.93904054e-03  4.68928032e-02  8.69194046e-03\n",
      "  1.88648272e-02  1.64884105e-02 -1.33191913e-01  1.18602000e-01\n",
      " -5.99169219e-03 -3.07412576e-02  1.23980613e-02 -2.51414767e-03\n",
      "  1.53518850e-02 -2.42204638e-03  1.81975737e-02  2.91568902e-03\n",
      "  5.37608787e-02  6.41017109e-02  2.24329438e-03 -2.87112426e-02\n",
      "  3.61746624e-02 -5.16132079e-02  6.22881623e-03 -6.95622936e-02\n",
      " -2.40000933e-02  8.77246168e-03  2.35458690e-04 -1.00001851e-02\n",
      " -3.03595178e-02 -2.21031588e-02 -3.01458575e-02  4.54219766e-02\n",
      " -4.10995483e-02  1.35458745e-02 -5.33247143e-02 -4.03209291e-02\n",
      "  2.09988877e-02  3.24746780e-02 -3.96741740e-02 -1.08051142e-02\n",
      " -3.12120118e-03  5.65311965e-03 -5.03480807e-02  3.01816911e-01\n",
      " -7.77511150e-02  5.67459390e-02  7.65911341e-02 -8.95579755e-02\n",
      "  9.26345587e-03  3.07867117e-02 -1.38225744e-03 -1.13371862e-02\n",
      " -4.48554680e-02 -6.16399106e-03  6.15082728e-03 -2.74915323e-02\n",
      "  1.57007594e-02 -2.81811077e-02  6.14589918e-03 -6.88803941e-02\n",
      "  1.86781399e-02 -6.27418840e-03  1.05892748e-01 -4.60451692e-02\n",
      "  1.63523406e-02  2.68719625e-02 -2.61658821e-02 -4.58459789e-03\n",
      "  5.57402074e-02 -5.32158874e-02  1.01258837e-01  9.18980241e-02\n",
      "  1.17919473e-02  6.28960058e-02  8.30772985e-03  1.11551642e-01\n",
      " -3.04661151e-02 -1.87373441e-02  7.32473517e-03  2.23601498e-02\n",
      "  3.25697730e-03 -1.99769139e-02  4.07716036e-02 -3.65959778e-02\n",
      "  1.11832330e-02 -9.79253724e-02 -4.67708893e-02 -1.02178894e-01\n",
      "  5.42596728e-03  2.68705171e-02 -5.62200584e-02  1.60805061e-02\n",
      " -1.08331358e-02 -7.46754091e-03 -3.94262448e-02  5.07000387e-02\n",
      " -2.43599247e-03 -2.06446294e-02 -7.24236527e-03  4.69797216e-02\n",
      "  2.15595979e-02  2.56899018e-02  4.73817736e-02  5.22456095e-02\n",
      " -6.41993433e-03  1.24270851e-02 -1.58190168e-02 -3.53246718e-03\n",
      " -4.98229191e-02 -1.23372748e-01 -1.38454081e-03 -8.16154666e-03\n",
      " -1.24719134e-02  1.27682658e-02  3.26665230e-02  6.51964033e-03\n",
      " -2.67273802e-02  3.85723859e-02  1.17717616e-01  1.57225039e-02\n",
      "  1.86455566e-02  6.62051886e-02 -2.49131117e-02  8.62909202e-03\n",
      "  3.48749012e-02 -3.76986302e-02 -8.26261006e-03  8.31103977e-03\n",
      "  1.37840062e-02 -3.93928401e-02 -2.87037902e-02 -7.15342723e-03\n",
      "  2.24384922e-03  2.79099680e-02  2.71385517e-02  4.58772369e-02\n",
      " -3.75539251e-02 -7.75863370e-03 -4.82653454e-02  2.24829316e-02\n",
      "  5.25270775e-02 -6.72100186e-02  1.34594021e-02 -3.11646797e-02\n",
      "  8.39875564e-02 -3.38000897e-03  1.04087517e-02  4.12140712e-02\n",
      "  6.63032085e-02  8.11193418e-03  2.52337269e-02 -6.53146952e-03\n",
      "  6.83093965e-02 -2.08245311e-03 -4.35978770e-02 -9.93236713e-03\n",
      "  8.11641812e-02 -1.75885658e-03  1.48187310e-03 -6.12671673e-02\n",
      " -1.90826412e-02  5.03852069e-02 -4.59591448e-02  3.64943370e-02\n",
      " -3.69790173e-03 -5.65033127e-03 -9.19907913e-02 -2.72817582e-01\n",
      "  6.07910827e-02 -1.61014739e-02 -2.74753645e-02  3.53532960e-03\n",
      " -6.16691122e-03  4.86973226e-02 -3.66492458e-02  1.09947406e-01\n",
      "  4.78209043e-03  4.31839079e-02 -7.94479549e-02  3.18976119e-02\n",
      " -3.00815143e-02  1.80540048e-02  5.47958799e-02 -8.33635777e-03\n",
      " -1.08375130e-02 -8.92530847e-03 -1.04989270e-02  2.08437853e-02\n",
      " -4.15893830e-03  9.54126660e-03 -3.88760604e-02 -1.89147855e-03\n",
      " -4.26448509e-02  1.50720313e-01  1.46018088e-01  6.41127443e-03\n",
      "  1.33572211e-02  4.48213406e-02  6.60796883e-03  1.60761189e-03\n",
      " -1.81174546e-01  2.61972211e-02  6.83027431e-02 -1.38523271e-02\n",
      " -6.43036887e-02 -1.00401603e-01 -1.72150787e-02  5.52336313e-03\n",
      "  6.77627139e-03 -4.44818474e-03 -4.00710590e-02 -9.35868360e-03\n",
      " -9.40444916e-02 -4.11411002e-02 -1.38224354e-02 -5.36406152e-02\n",
      " -1.35722943e-02  9.83819016e-04 -7.43353521e-05  2.76839100e-02\n",
      "  5.76662198e-02 -2.52357069e-02 -6.34068102e-02  1.04658538e-02\n",
      " -3.87894362e-02 -3.59659567e-02  5.14375940e-02  4.58200648e-03\n",
      "  2.44435295e-02 -9.84088634e-04 -5.95263671e-03 -4.57678139e-02\n",
      "  6.33374825e-02 -3.18805464e-02 -2.59226300e-02  8.90967548e-02\n",
      " -2.23556105e-02 -2.28121653e-02 -3.16789709e-02  3.98066193e-02\n",
      " -5.20041585e-03  3.28155905e-02  3.52165811e-02 -1.87775280e-04\n",
      "  1.32266711e-02 -5.43552116e-02 -5.94479367e-02  7.22019561e-03\n",
      " -5.66423126e-03  6.59941956e-02  5.58930170e-03  1.33473463e-02\n",
      "  6.78270683e-02  4.99403700e-02 -3.05749644e-02  1.59924217e-02\n",
      " -3.36170234e-02 -2.88485587e-02 -2.91215419e-03 -1.23735499e-02\n",
      " -8.45806226e-02  2.69654058e-02 -3.98771949e-02 -2.96614021e-01\n",
      "  5.76999336e-02 -1.95296649e-02 -1.03638007e-03 -2.88452469e-02\n",
      "  5.46621121e-02  4.43069190e-02  2.07050126e-02 -5.87930940e-02\n",
      "  1.73899196e-02 -5.90409003e-02  3.18925157e-02  2.44015791e-02\n",
      "  8.20826553e-03 -2.29257178e-02  3.75748761e-02  1.03224851e-02\n",
      " -6.28589280e-03  7.08621880e-03  7.91991875e-03 -3.34007503e-03\n",
      "  6.96360599e-03  1.86309844e-01 -4.55221720e-02  3.79985049e-02\n",
      "  4.62052114e-02 -1.75786782e-02  2.13359147e-02 -4.45414381e-03\n",
      "  3.94241186e-03 -4.37966585e-02 -7.20767945e-04  2.97210831e-02\n",
      " -2.61628628e-02  1.87387038e-02 -1.67528987e-02 -4.55069318e-02\n",
      " -1.55643169e-02  2.46400982e-02 -3.81163545e-02 -6.58555031e-02\n",
      "  3.28967385e-02 -4.03053463e-02  6.13708533e-02  4.95351925e-02\n",
      " -7.14956746e-02  1.08512826e-02 -1.51515203e-02 -9.56853200e-03\n",
      "  1.18796285e-02 -2.34576873e-02 -3.30263190e-02  9.05795023e-03\n",
      "  2.73241345e-02 -3.17034163e-02 -5.58008486e-03  2.13997141e-02\n",
      " -2.02633310e-02  2.82556508e-02 -6.36098459e-02  3.91589431e-03\n",
      " -1.15283020e-02  5.11194170e-02  5.04094101e-02  2.53525954e-02]\n"
     ]
    }
   ],
   "source": [
    "print(len(emb_text))\n",
    "print(len(emb_text[0]))\n",
    "print(len(emb_text[1]))\n",
    "print(emb_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the people who were shovelling away on the housetops were jovial and full of glee; calling out to one another from the parapets, and now and then exchanging a facetious snowball\ufeff\u2014better-natured missile far than many a wordy jest\ufeff\u2014laughing heartily if it went right, and not less heartily if it went wrong. The poulterers\u2019 shops were still half open, and the fruiterers\u2019 were radiant in their glory. There were great, round, potbellied baskets of chestnuts, shaped like the waistcoats of jolly old gentlemen, lolling at the doors, and tumbling out into the street in their apoplectic opulence: There were ruddy, brown-faced, broad-girthed Spanish onions, shining in the fatness of their growth like Spanish friars, and winking from their shelves in wanton slyness at the girls as they went by, and glanced demurely at the hung-up mistletoe. There were pears and apples clustered high in blooming pyramids; there were bunches of grapes, made, in the shopkeepers\u2019 benevolence, to dangle from conspicuous hooks that\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "# TODO: Prepare the chunks for inserting into Chroma\n",
    "# Extract the text\n",
    "texts = [ c.page_content for c in chunks ]\n",
    "print(texts[100])\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['277d0bb8', 'bbe6d999', 'a07de751', 'e60d2e0a', 'b4a98aa7', '01b6444e', '49d46dce', '092ced88', '95c3810f', '1d8829e4', '65af40ca', 'd17ec7a5', 'bb1bbdbb', 'aa5514f7', '5e80c94a', '0fc79f83', '1ad4e15f', 'e387ff1b', '5db028d6', 'ff34017d', 'bd561d9c', '35c5b8d2', 'fec09abd', '773e74b7', 'b20689bb', '763d87d0', 'e2789d3b', 'b2078cdf', '26da3249', 'fa98828d', '14407106', '5327d3c1', 'defba9c2', '0eaec760', '76ceebd8', '2c2f9df7', '0901abd1', 'af7a64dc', 'b24dec98', '9f92d87b', '94cc593f', '301851ad', 'b43a424d', 'f3fff753', '8d4c56cd', 'd609844d', '7fe71b78', 'ddeebeab', '0c7a6173', 'b99640e6', '233e05ab', '9c506a5f', 'd8f2cb55', '1d642b35', 'b8a0ca1b', 'ef5f377b', 'a4df0832', '2238b7d7', 'b5e4ce98', '702c156b', '2c693bb8', '2dbae0de', '6a43dfbd', 'a65c5a28', '58e595df', 'af72ddae', '9c4913ae', '3406831b', '0a7cd4bf', '09ea8047', '8c555549', '57f15a08', 'f755bbdf', '0399c18f', '74eba842', '01934995', 'a6381159', 'db380b09', 'b49e0f47', '8107e3e7', 'd3c70d23', '6c2dcd9e', 'df832793', '428fc12e', 'b9166830', '9b1a5f85', 'fc3b5bde', '13a77f23', '9b6f17f2', '7f0d2421', 'a06d957f', '75c7509a', '3a69fffe', '48e929e6', 'cf9d4ffc', '5e0b01b4', '6d0c69cd', '9638a9ee', 'a07a3957', '95d24063', '9537da9d', 'c4c66941', '02a63979', '423f89ba', 'cd52799e', 'bcbb2e20', 'a65e60ab', 'a9cc5821', '1edc94f1', 'c3306757', '9519c52f', '4b24ab4e', 'ef2a49fd', 'a7fc1d1f', 'b79f46bf', '6c9c48f2', 'ba628598', 'a173c964', '61571746', '493459c4', '7a8b4bca', '5fbcab6a', 'c9da99e3', '734d84e3', '4b01f296', 'fa4f2118', '76ea7998', '6f0f4c40', '2bdbad13', '2eaf3424', '52f7a8b5', '4cd28523', '009e30fb', 'ef9d2859', '68f1c5f3', 'abc5b750', '73b99986', 'cf79b7aa', 'ed038bb4', '0ef39147', '3a97a627', 'a9d62646', '7588e81c', '1066ad41', '2ffdde56', 'c8b4fc95', '8cc30942', '96ec9c9b', 'ab4b7dc7', '0b3deb33', 'bb64db67', 'a6c18a83', '8c3eacdf', '9e5e7e04', '731a09c1', '4fcb2f46', '1a7df15a', '649c4ccb', 'b6fe0494', 'a1d9f8c8', 'c4700e69', 'c79d85dc', '1ac62ff5', 'e9994b4e', '433e820e', 'fab2e2e2', '912274bb', '159be545', '78afe94d', 'b43e693f', '3f8540a5', 'df1402c4', 'ba58c4ce', '2e64c666', '6c141e1a', '14dbdf94', 'a22fc711', 'e0a0ea11', 'ae0f7e94', '5ca0536b', '38751c30', '04906012', 'f0858321', '8153fd60', 'be22753d', 'f8117b44', '6c040485', 'ecd3c977', 'e8ffce82', '0aca46d4', '94888deb', '6412d910', 'e27d4b4f', '60c09c2c', '9f17bdd7', '94cc9e0e', '21e7602a', 'a1d9bd60', '17e99ec0', '1480cce1', '01bc0299', '71a517a0', '01f15d44']\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "text_ids = [  str(uuid4())[:8] for _ in range(len(texts))]\n",
    "print(text_ids)\n",
    "print(len(text_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create ephemeral Chroma client and save chunks\n",
    "col_name = 'carol'\n",
    "\n",
    "# Create a the chromadb client\n",
    "ch_client = chromadb.Client()\n",
    "\n",
    "# drop the table\n",
    "try:\n",
    "   ch_client.delete_collection(col_name)\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Insert the texts into the database\n",
    "carol_col = ch_client.create_collection(\n",
    "   name = col_name,\n",
    "   embedding_function=chroma_embed_func\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert the docs into the collection\n",
    "carol_col.add(\n",
    "   documents = texts,\n",
    "   ids = text_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of documents in collection \n",
    "print(carol_col.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['ddeebeab', 'e60d2e0a', '773e74b7', 'fa98828d', '5327d3c1']], 'embeddings': None, 'documents': [['Marley\u2019s Ghost bothered him exceedingly. Every time he resolved within himself, after mature inquiry that it was all a dream, his mind flew back again, like a strong spring released, to its first position, and presented the same problem to be worked all through, \u201cWas it a dream or not?\u201d\\n\\nScrooge lay in this state until the chime had gone three-quarters more, when he remembered, on a sudden, that the Ghost had warned him of a visitation when the bell tolled one. He resolved to lie awake until the hour was passed; and, considering that he could no more go to sleep than go to heaven, this was, perhaps, the wisest resolution in his power.\\n\\nThe quarter was so long, that he was more than once convinced he must have sunk into a doze unconsciously, and missed the clock. At length it broke upon his listening ear.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cA quarter past,\u201d said Scrooge, counting.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cHalf past,\u201d said Scrooge.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cA quarter to it,\u201d said Scrooge.\\n\\n\u201cDing, dong!\u201d', 'The mention of Marley\u2019s funeral brings me back to the point I started from. There is no doubt that Marley was dead. This must be distinctly understood, or nothing wonderful can come of the story I am going to relate. If we were not perfectly convinced that Hamlet\u2019s father died before the play began, there would be nothing more remarkable in his taking a stroll at night, in an easterly wind, upon his own ramparts, than there would be in any other middle-aged gentleman rashly turning out after dark in a breezy spot\\ufeff\u2014say St. Paul\u2019s Churchyard, for instance\\ufeff\u2014literally to astonish his son\u2019s weak mind.\\n\\nScrooge never painted out Old Marley\u2019s name. There it stood, years afterwards, above the warehouse door: Scrooge and Marley. The firm was known as Scrooge and Marley. Sometimes people new to the business called Scrooge Scrooge, and sometimes Marley, but he answered to both names. It was all the same to him.', 'Marley\u2019s face. It was not in impenetrable shadow, as the other objects in the yard were, but had a dismal light about it, like a bad lobster in a dark cellar. It was not angry or ferocious, but looked at Scrooge as Marley used to look; with ghostly spectacles turned up on its ghostly forehead. The hair was curiously stirred, as if by breath or hot air; and, though the eyes were wide open, they were perfectly motionless. That, and its livid colour, made it horrible; but its horror seemed to be in spite of the face, and beyond its control, rather than a part of its own expression.\\n\\nAs Scrooge looked fixedly at this phenomenon, it was a knocker again.\\n\\nTo say that he was not startled, or that his blood was not conscious of a terrible sensation to which it had been a stranger from infancy, would be untrue. But he put his hand upon the key he had relinquished, turned it sturdily, walked in, and lighted his candle.', 'The cellar door flew open with a booming sound, and then he heard the noise much louder on the floors below; then coming up the stairs; then coming straight towards his door.\\n\\n\u201cIt\u2019s humbug still!\u201d said Scrooge. \u201cI won\u2019t believe it.\u201d\\n\\nHis colour changed, though, when, without a pause, it came on through the heavy door and passed into the room before his eyes. Upon its coming in, the dying flame leaped up, as though it cried, \u201cI know him! Marley\u2019s Ghost!\u201d and fell again.', '\u201cHow now!\u201d said Scrooge, caustic and cold as ever. \u201cWhat do you want with me?\u201d\\n\\n\u201cMuch!\u201d\\ufeff\u2014Marley\u2019s voice; no doubt about it.\\n\\n\u201cWho are you?\u201d\\n\\n\u201cAsk me who I was.\u201d\\n\\n\u201cWho were you, then?\u201d said Scrooge, raising his voice. \u201cYou\u2019re particular, for a shade.\u201d He was going to say \u201cto a shade,\u201d but substituted this, as more appropriate.\\n\\n\u201cIn life I was your partner, Jacob Marley.\u201d\\n\\n\u201cCan you\\ufeff\u2014can you sit down?\u201d asked Scrooge, looking doubtfully at him.\\n\\n\u201cI can.\u201d\\n\\n\u201cDo it, then.\u201d\\n\\nScrooge asked the question, because he didn\u2019t know whether a ghost so transparent might find himself in a condition to take a chair; and felt that in the event of its being impossible, it might involve the necessity of an embarrassing explanation. But the Ghost sat down on the opposite side of the fireplace, as if he were quite used to it.\\n\\n\u201cYou don\u2019t believe in me,\u201d observed the Ghost.\\n\\n\u201cI don\u2019t,\u201d said Scrooge.\\n\\n\u201cWhat evidence would you have of my reality beyond that of your own senses?\u201d\\n\\n\u201cI don\u2019t know,\u201d said Scrooge.']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None, None, None, None]], 'distances': [[0.31335413455963135, 0.3227928876876831, 0.3263511657714844, 0.3307737112045288, 0.3326452374458313]]}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Query collection \n",
    "query = \"What happened Marley?\"\n",
    "\n",
    "\n",
    "results = carol_col.query(\n",
    "   query_texts=[ query ],\n",
    "   n_results=5\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marley\u2019s Ghost bothered him exceedingly. Every time he resolved within himself, after mature inquiry that it was all a dream, his mind flew back again, like a strong spring released, to its first position, and presented the same problem to be worked all through, \u201cWas it a dream or not?\u201d\\n\\nScrooge lay in this state until the chime had gone three-quarters more, when he remembered, on a sudden, that the Ghost had warned him of a visitation when the bell tolled one. He resolved to lie awake until the hour was passed; and, considering that he could no more go to sleep than go to heaven, this was, perhaps, the wisest resolution in his power.\\n\\nThe quarter was so long, that he was more than once convinced he must have sunk into a doze unconsciously, and missed the clock. At length it broke upon his listening ear.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cA quarter past,\u201d said Scrooge, counting.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cHalf past,\u201d said Scrooge.\\n\\n\u201cDing, dong!\u201d\\n\\n\u201cA quarter to it,\u201d said Scrooge.\\n\\n\u201cDing, dong!\u201d']\n",
      "['The mention of Marley\u2019s funeral brings me back to the point I started from. There is no doubt that Marley was dead. This must be distinctly understood, or nothing wonderful can come of the story I am going to relate. If we were not perfectly convinced that Hamlet\u2019s father died before the play began, there would be nothing more remarkable in his taking a stroll at night, in an easterly wind, upon his own ramparts, than there would be in any other middle-aged gentleman rashly turning out after dark in a breezy spot\\ufeff\u2014say St. Paul\u2019s Churchyard, for instance\\ufeff\u2014literally to astonish his son\u2019s weak mind.\\n\\nScrooge never painted out Old Marley\u2019s name. There it stood, years afterwards, above the warehouse door: Scrooge and Marley. The firm was known as Scrooge and Marley. Sometimes people new to the business called Scrooge Scrooge, and sometimes Marley, but he answered to both names. It was all the same to him.']\n",
      "['Marley\u2019s face. It was not in impenetrable shadow, as the other objects in the yard were, but had a dismal light about it, like a bad lobster in a dark cellar. It was not angry or ferocious, but looked at Scrooge as Marley used to look; with ghostly spectacles turned up on its ghostly forehead. The hair was curiously stirred, as if by breath or hot air; and, though the eyes were wide open, they were perfectly motionless. That, and its livid colour, made it horrible; but its horror seemed to be in spite of the face, and beyond its control, rather than a part of its own expression.\\n\\nAs Scrooge looked fixedly at this phenomenon, it was a knocker again.\\n\\nTo say that he was not startled, or that his blood was not conscious of a terrible sensation to which it had been a stranger from infancy, would be untrue. But he put his hand upon the key he had relinquished, turned it sturdily, walked in, and lighted his candle.']\n",
      "['The cellar door flew open with a booming sound, and then he heard the noise much louder on the floors below; then coming up the stairs; then coming straight towards his door.\\n\\n\u201cIt\u2019s humbug still!\u201d said Scrooge. \u201cI won\u2019t believe it.\u201d\\n\\nHis colour changed, though, when, without a pause, it came on through the heavy door and passed into the room before his eyes. Upon its coming in, the dying flame leaped up, as though it cried, \u201cI know him! Marley\u2019s Ghost!\u201d and fell again.']\n",
      "['\u201cHow now!\u201d said Scrooge, caustic and cold as ever. \u201cWhat do you want with me?\u201d\\n\\n\u201cMuch!\u201d\\ufeff\u2014Marley\u2019s voice; no doubt about it.\\n\\n\u201cWho are you?\u201d\\n\\n\u201cAsk me who I was.\u201d\\n\\n\u201cWho were you, then?\u201d said Scrooge, raising his voice. \u201cYou\u2019re particular, for a shade.\u201d He was going to say \u201cto a shade,\u201d but substituted this, as more appropriate.\\n\\n\u201cIn life I was your partner, Jacob Marley.\u201d\\n\\n\u201cCan you\\ufeff\u2014can you sit down?\u201d asked Scrooge, looking doubtfully at him.\\n\\n\u201cI can.\u201d\\n\\n\u201cDo it, then.\u201d\\n\\nScrooge asked the question, because he didn\u2019t know whether a ghost so transparent might find himself in a condition to take a chair; and felt that in the event of its being impossible, it might involve the necessity of an embarrassing explanation. But the Ghost sat down on the opposite side of the fireplace, as if he were quite used to it.\\n\\n\u201cYou don\u2019t believe in me,\u201d observed the Ghost.\\n\\n\u201cI don\u2019t,\u201d said Scrooge.\\n\\n\u201cWhat evidence would you have of my reality beyond that of your own senses?\u201d\\n\\n\u201cI don\u2019t know,\u201d said Scrooge.']\n"
     ]
    }
   ],
   "source": [
    "for id in results['ids'][0]:\n",
    "   result = carol_col.get(id)\n",
    "   print(result['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer LLM\n",
    "In this exercise you will implement a question and answer LLM for the 'A Christmas Carol' book that you have chunked and saved. \n",
    "\n",
    "The workflow is as follows:\n",
    "1. Assume you ask the following question regarding the book eg. `\"Who is Scrooge?\"`?\n",
    "2. Query the relevant context from Chroma with the question or facts from the question.\n",
    "3. Combine the question and the top 5 context return by Chroma into a prompt \n",
    "4. Use `google/flan-t5-base` to answer the question.\n",
    "\n",
    "Look through the FLAN templates in [Github](https://github.com/google-research/FLAN/blob/main/flan/templates.py) and select an appropriate template for this workshop.\n",
    "\n",
    "Do not worry about the accuracy of the result. Focus on implementing the solution. We will discuss the nuances of the solution at the end of the workshop.\n",
    "\n",
    "Use your RAG workflow to answer the provided questions in `questions_for_rag.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f505fdefd5b4110a81deb0e77debcf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e311a8963dc54f0889cd6cdd33423ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aff28793b8741a99a64370eced664ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c220e9a0344c9182de9a9f73aaae2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65ed9a270554d5abc302f862cbf89ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737320342468467cb47b7928eecb310a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665785bb4f6747899396edc159bdba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Your code \n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Cratchit's youngest son is ill.\n"
     ]
    }
   ],
   "source": [
    "# Extract the core ideas of the question \n",
    "question = \"What is the name of Scrooge's underpaid clerk?\"\n",
    "#question = \"Who was Scrooge's deceased business partner?\"\n",
    "#question = \"Who was Scrooge engaged to in his youth, and why did she leave him?\"\n",
    "question = \"What is the name of Bob Cratchit's youngest son who is ill?\"\n",
    "#question = \"What does Scrooge see written on the gravestone that frightens him into changing his ways?\"\n",
    "#question = \" What is Scrooge's response when his nephew Fred invites him to Christmas dinner at the beginning of the story?\"\n",
    "#question = \" What specific, generous act does Scrooge perform for the Cratchit family on Christmas morning?\"\n",
    "\n",
    "prompt = f\"{question}\\n\\nWhat is sentence that verbalizes this data?\"\n",
    "#prompt = f\"{question}\\n\\nWhat data can be extracted from this sentence?\"\n",
    "#prompt = f\"Generate an approximately fifteen-word sentence that describes all this data: {question}\"\n",
    "\n",
    "# convert to a statement\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "enc_answer = model.generate(enc_prompt.input_ids)\n",
    "answer = tokenizer.decode(enc_answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She hurried out to meet him; and little Bob in his comforter\ufeff\u2014he had need of it, poor fellow\ufeff\u2014came in. His tea was ready for him on the hob, and they all tried who should help him to it most. Then the two young Cratchits got upon his knees, and laid, each child, a little cheek against his face, as if they said, \u201cDon\u2019t mind it, father. Don\u2019t be grieved!\u201d\n",
      "\n",
      "Bob was very cheerful with them, and spoke pleasantly to all the family. He looked at the work upon the table, and praised the industry and speed of Mrs. Cratchit and the girls. They would be done long before Sunday, he said.\n",
      "\n",
      "\u201cSunday! You went today, then, Robert?\u201d said his wife.\n",
      "\n",
      "\u201cYes, my dear,\u201d returned Bob. \u201cI wish you could have gone. It would have done you good to see how green a place it is. But you\u2019ll see it often. I promised him that I would walk there on a Sunday. My little, little child!\u201d cried Bob. \u201cMy little child!\u201dSo Martha hid herself, and in came little Bob, the father, with at least three feet of comforter, exclusive of the fringe, hanging down before him, and his threadbare clothes darned up and brushed to look seasonable, and Tiny Tim upon his shoulder. Alas for Tiny Tim, he bore a little crutch, and had his limbs supported by an iron frame!\n",
      "\n",
      "\u201cWhy, where\u2019s our Martha?\u201d cried Bob Cratchit, looking round.\n",
      "\n",
      "\u201cNot coming,\u201d said Mrs. Cratchit.\n",
      "\n",
      "\u201cNot coming!\u201d said Bob, with a sudden declension in his high spirits; for he had been Tim\u2019s blood-horse all the way from church, and had come home rampant. \u201cNot coming upon Christmas Day!\u201d\n",
      "\n",
      "Martha didn\u2019t like to see him disappointed, if it were only in joke; so she came out prematurely from behind the closet door, and ran into his arms, while the two young Cratchits hustled Tiny Tim, and bore him off into the washhouse, that he might hear the pudding singing in the copper.\u201cWhat has ever got your precious father, then?\u201d said Mrs. Cratchit. \u201cAnd your brother, Tiny Tim? And Martha warn\u2019t as late last Christmas Day by half an hour!\u201d\n",
      "\n",
      "\u201cHere\u2019s Martha, mother!\u201d said a girl, appearing as she spoke.\n",
      "\n",
      "\u201cHere\u2019s Martha, mother!\u201d cried the two young Cratchits. \u201cHurrah! There\u2019s such a goose, Martha!\u201d\n",
      "\n",
      "\u201cWhy, bless your heart alive, my dear, how late you are!\u201d said Mrs. Cratchit, kissing her a dozen times, and taking off her shawl and bonnet for her with officious zeal.\n",
      "\n",
      "\u201cWe\u2019d a deal of work to finish up last night,\u201d replied the girl, \u201cand had to clear away this morning, mother!\u201d\n",
      "\n",
      "\u201cWell! never mind so long as you are come,\u201d said Mrs. Cratchit. \u201cSit ye down before the fire, my dear, and have a warm, Lord bless ye!\u201d\n",
      "\n",
      "\u201cNo, no! There\u2019s father coming,\u201d cried the two young Cratchits, who were everywhere at once. \u201cHide, Martha, hide!\u201d\n"
     ]
    }
   ],
   "source": [
    "# TODO Your code\n",
    "# FIX: Use the original question for search, not the reformulated answer\n",
    "# The reformulation loses important details from the question\n",
    "context = \"\"\n",
    "results = carol_col.query(\n",
    "   query_texts=[ question ],\n",
    "   n_results=3\n",
    ")\n",
    "for id in results['ids'][0]:\n",
    "   result = carol_col.get(id)\n",
    "   context += result['documents'][0]\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer based on context:\n",
      "\n",
      "She hurried out to meet him; and little Bob in his comforter\ufeff\u2014he had need of it, poor fellow\ufeff\u2014came in. His tea was ready for him on the hob, and they all tried who should help him to it most. Then the two young Cratchits got upon his knees, and laid, each child, a little cheek against his face, as if they said, \u201cDon\u2019t mind it, father. Don\u2019t be grieved!\u201d\n",
      "\n",
      "Bob was very cheerful with them, and spoke pleasantly to all the family. He looked at the work upon the table, and praised the industry and speed of Mrs. Cratchit and the girls. They would be done long before Sunday, he said.\n",
      "\n",
      "\u201cSunday! You went today, then, Robert?\u201d said his wife.\n",
      "\n",
      "\u201cYes, my dear,\u201d returned Bob. \u201cI wish you could have gone. It would have done you good to see how green a place it is. But you\u2019ll see it often. I promised him that I would walk there on a Sunday. My little, little child!\u201d cried Bob. \u201cMy little child!\u201dSo Martha hid herself, and in came little Bob, the father, with at least three feet of comforter, exclusive of the fringe, hanging down before him, and his threadbare clothes darned up and brushed to look seasonable, and Tiny Tim upon his shoulder. Alas for Tiny Tim, he bore a little crutch, and had his limbs supported by an iron frame!\n",
      "\n",
      "\u201cWhy, where\u2019s our Martha?\u201d cried Bob Cratchit, looking round.\n",
      "\n",
      "\u201cNot coming,\u201d said Mrs. Cratchit.\n",
      "\n",
      "\u201cNot coming!\u201d said Bob, with a sudden declension in his high spirits; for he had been Tim\u2019s blood-horse all the way from church, and had come home rampant. \u201cNot coming upon Christmas Day!\u201d\n",
      "\n",
      "Martha didn\u2019t like to see him disappointed, if it were only in joke; so she came out prematurely from behind the closet door, and ran into his arms, while the two young Cratchits hustled Tiny Tim, and bore him off into the washhouse, that he might hear the pudding singing in the copper.\u201cWhat has ever got your precious father, then?\u201d said Mrs. Cratchit. \u201cAnd your brother, Tiny Tim? And Martha warn\u2019t as late last Christmas Day by half an hour!\u201d\n",
      "\n",
      "\u201cHere\u2019s Martha, mother!\u201d said a girl, appearing as she spoke.\n",
      "\n",
      "\u201cHere\u2019s Martha, mother!\u201d cried the two young Cratchits. \u201cHurrah! There\u2019s such a goose, Martha!\u201d\n",
      "\n",
      "\u201cWhy, bless your heart alive, my dear, how late you are!\u201d said Mrs. Cratchit, kissing her a dozen times, and taking off her shawl and bonnet for her with officious zeal.\n",
      "\n",
      "\u201cWe\u2019d a deal of work to finish up last night,\u201d replied the girl, \u201cand had to clear away this morning, mother!\u201d\n",
      "\n",
      "\u201cWell! never mind so long as you are come,\u201d said Mrs. Cratchit. \u201cSit ye down before the fire, my dear, and have a warm, Lord bless ye!\u201d\n",
      "\n",
      "\u201cNo, no! There\u2019s father coming,\u201d cried the two young Cratchits, who were everywhere at once. \u201cHide, Martha, hide!\u201d\n",
      "\n",
      "What is the name of Bob Cratchit's youngest son who is ill?\n"
     ]
    }
   ],
   "source": [
    "question_prompt = f\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\n",
    "print(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of Bob Cratchit's youngest son who is ill?\n",
      "Tiny Tim\n"
     ]
    }
   ],
   "source": [
    "# TODO Your code\n",
    "enc_query_prompt = tokenizer(question_prompt, return_tensors='pt')\n",
    "\n",
    "enc_query_answer = model.generate(enc_query_prompt.input_ids)\n",
    "\n",
    "query_answer = tokenizer.decode(enc_query_answer[0], skip_special_tokens=True)\n",
    "\n",
    "print(question)\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "1. How did your solution perform?\n",
    "2. Where do you think are the issues?\n",
    "3. How can you improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on analyzing the notebook, here are the key issues with this RAG implementation:\n",
    "\n",
    "  1. Question Reformulation Adds Complexity & Errors\n",
    "\n",
    "  # Cell 18: Converts question to statement first\n",
    "  question = \"What is the name of Bob Cratchit's youngest son who is ill?\"\n",
    "  # Becomes: \"Bob Cratchit's youngest son is ill.\"\n",
    "  Problems:\n",
    "  - Extra LLM call adds latency and cost\n",
    "  - Can lose important question details (e.g., asking for a NAME gets lost)\n",
    "  - The reformulation might be wrong or incomplete\n",
    "  - Why not just search with the original question?\n",
    "\n",
    "  2. Very Limited Context (Only 3 Chunks)\n",
    "\n",
    "  # Cell 19: Only retrieves 3 results\n",
    "  # FIX APPLIED: Now uses question instead of reformulated answer\n",
    "  results = carol_col.query(query_texts=[question], n_results=3)\n",
    "  Issues:\n",
    "  - 3 chunks \u00d7 1024 chars = ~3,000 characters total context\n",
    "  - Important information might be in chunks 4-10\n",
    "  - No diversity in retrieval (all chunks might be from same scene)\n",
    "\n",
    "  3. Small Model Limitations (FLAN-T5-Base)\n",
    "\n",
    "  - Only 250M parameters - limited reasoning capability\n",
    "  - Struggles with complex questions requiring inference\n",
    "  - Short output length limits detailed answers\n",
    "  - No instruction fine-tuning for RAG tasks specifically\n",
    "\n",
    "  4. Chunking Strategy Issues\n",
    "\n",
    "  chunk_size = 1024\n",
    "  chunk_overlap = 50  # Only 5% overlap!\n",
    "  Problems:\n",
    "  - 50-character overlap is very small (just ~10 words)\n",
    "  - Character-based chunking can split mid-sentence\n",
    "  - No semantic awareness (might split a conversation)\n",
    "  - 1024 chars might cut important multi-paragraph context\n",
    "\n",
    "  5. No Answer Validation or Confidence Scoring\n",
    "\n",
    "  - Doesn't check if the answer is actually in the context\n",
    "  - No confidence scores shown to user\n",
    "  - Could hallucinate if context doesn't contain answer\n",
    "  - No fallback for \"I don't know\"\n",
    "\n",
    "  6. Embedding Model Limitations\n",
    "\n",
    "  embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "  - Only 384 dimensions (smaller models = less nuanced)\n",
    "  - Might not capture subtle semantic differences\n",
    "  - Same embedding for questions and passages (not optimized)\n",
    "\n",
    "  7. Ephemeral Database (Lost on Restart)\n",
    "\n",
    "  ch_client = chromadb.Client()  # In-memory only!\n",
    "  - All embeddings lost when notebook restarts\n",
    "  - Must re-embed entire book every time (~6 minutes)\n",
    "  - No persistence for production use\n",
    "\n",
    "  8. No Metadata or Filtering\n",
    "\n",
    "  - Can't filter by chapter, character name, or scene\n",
    "  - No source attribution (which chapter is the answer from?)\n",
    "  - Can't do temporal reasoning (\"What happened BEFORE Marley appeared?\")\n",
    "\n",
    "  9. Single Query Strategy\n",
    "\n",
    "  - Only one attempt at retrieval\n",
    "  - No query expansion (synonyms, rephrasings)\n",
    "  - No hybrid search (keyword + semantic)\n",
    "  - Misses the HyDE (Hypothetical Document Embeddings) opportunity\n",
    "\n",
    "  10. Prompt Engineering Issues\n",
    "\n",
    "  question_prompt = f\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\n",
    "  - Very basic prompt - no instructions about:\n",
    "    - Answer length\n",
    "    - What to do if answer not found\n",
    "    - How to cite sources\n",
    "    - Format expectations\n",
    "\n",
    "  Example of Failure Mode\n",
    "\n",
    "  If you asked: \"How did Scrooge change?\"\n",
    "\n",
    "  1. Reformulation might produce: \"Scrooge changed\" (loses the \"HOW\")\n",
    "  2. Top 3 chunks might all be about one change, missing others\n",
    "  3. FLAN-T5-base might give overly simplistic answer\n",
    "  4. No way to know which parts of the book the answer came from\n",
    "\n",
    "  How to Improve (Quick Wins)\n",
    "\n",
    "  1. Skip the reformulation - search with original question\n",
    "  2. Increase context - retrieve 5-10 chunks\n",
    "  3. Better chunking - use 512 overlap, semantic splitting\n",
    "  4. Add persistence - use chromadb.PersistentClient()\n",
    "  5. Improve prompt - add instructions for \"unknown\" cases\n",
    "  6. Use larger model - FLAN-T5-large or modern LLM\n",
    "  7. Add reranking - re-score top-k results with cross-encoder\n",
    "  8. Show sources - return chunk IDs/page numbers with answer\n",
    "\n",
    "  The current approach works for simple factual questions but struggles with complex reasoning, multi-hop questions, or\n",
    "  questions requiring synthesis across multiple parts of the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellm-2025-nov17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}