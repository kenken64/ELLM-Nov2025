{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved RAG System for A Christmas Carol\n",
    "\n",
    "This notebook implements an **improved** RAG (Retrieval-Augmented Generation) system with the following enhancements:\n",
    "\n",
    "## Key Improvements:\n",
    "\n",
    "1. **Skip Reformulation** - Search directly with original question (simpler, faster)\n",
    "2. **Increased Context** - Retrieve 5-10 chunks instead of 3\n",
    "3. **Better Chunking** - Use 512 character overlap (was 50) for better context preservation\n",
    "4. **Persistent Storage** - Use ChromaDB PersistentClient to avoid re-embedding\n",
    "5. **Improved Prompt** - Add instructions for handling \"unknown\" cases\n",
    "6. **Larger Model** - Use FLAN-T5-large (780M params vs 250M)\n",
    "7. **Reranking** - Add cross-encoder reranking for better relevance\n",
    "8. **Source Attribution** - Show chunk IDs and metadata with answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredEPubLoader\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m embedding_functions\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement #3: Better Chunking Strategy\n",
    "\n",
    "**Changed from:**\n",
    "- chunk_size = 1024\n",
    "- chunk_overlap = 50 (only 5%!)\n",
    "\n",
    "**Changed to:**\n",
    "- chunk_size = 1024 (same)\n",
    "- chunk_overlap = 512 (50% overlap)\n",
    "\n",
    "**Why this matters:**\n",
    "- 512 character overlap ensures important context at chunk boundaries is preserved\n",
    "- Prevents conversations/paragraphs from being split awkwardly\n",
    "- Each chunk shares significant context with neighbors\n",
    "- Improves retrieval quality when answer spans multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document with improved chunking\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 512  # IMPROVED: 50% overlap instead of 5%\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "epub_loader = UnstructuredEPubLoader('./docs/charles-dickens_a-christmas-carol.epub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 274\n",
      "\n",
      "Sample chunk:\n",
      "page_content='“My time grows short,” observed the Spirit. “Quick!”\n",
      "\n",
      "This was not addressed to Scrooge, or to anyone whom he could see, but it produced an immediate effect. For again Scrooge saw himself. He was older now; a man in the prime of life. His face had not the harsh and rigid lines of later years; but it had begun to wear the signs of care and avarice. There was an eager, greedy, restless motion in the eye, which showed the passion that had taken root, and where the shadow of the growing tree would fall.\n",
      "\n",
      "He was not alone, but sat by the side of a fair young girl in a mourning dress: in whose eyes there were tears, which sparkled in the light that shone out of the Ghost of Christmas Past.\n",
      "\n",
      "“It matters little,” she said softly. “To you, very little. Another idol has displaced me; and, if it can cheer and comfort you in time to come as I would have tried to do, I have no just cause to grieve.”\n",
      "\n",
      "“What idol has displaced you?” he rejoined.\n",
      "\n",
      "“A golden one.”' metadata={'source': './docs/charles-dickens_a-christmas-carol.epub'}\n"
     ]
    }
   ],
   "source": [
    "# Split document\n",
    "chunks = epub_loader.load_and_split(text_splitter)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"\\nSample chunk:\\n{chunks[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding model (same as before - works well)\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "chroma_embed_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement #4: Persistent ChromaDB Storage\n",
    "\n",
    "**Changed from:**\n",
    "```python\n",
    "ch_client = chromadb.Client()  # Ephemeral - lost on restart\n",
    "```\n",
    "\n",
    "**Changed to:**\n",
    "```python\n",
    "ch_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Saved to disk\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- No need to re-embed documents every time\n",
    "- Much faster startup after first run\n",
    "- Production-ready storage\n",
    "- Can share database across multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts: 274\n",
      "Total IDs: 274\n",
      "Sample metadata: {'chunk_id': 0, 'source': './docs/charles-dickens_a-christmas-carol.epub', 'text_preview': 'Imprint\\n\\nThe Standard Ebooks logo.\\n\\nThis ebook is the product of many hours of hard work by voluntee...'}\n"
     ]
    }
   ],
   "source": [
    "# Prepare chunks for ChromaDB\n",
    "texts = [c.page_content for c in chunks]\n",
    "text_ids = [str(uuid4())[:8] for _ in range(len(texts))]\n",
    "\n",
    "# Store chunk metadata for source attribution (Improvement #8)\n",
    "metadatas = [\n",
    "    {\n",
    "        \"chunk_id\": i,\n",
    "        \"source\": chunks[i].metadata.get(\"source\", \"unknown\"),\n",
    "        \"text_preview\": texts[i][:100] + \"...\"\n",
    "    }\n",
    "    for i in range(len(texts))\n",
    "]\n",
    "\n",
    "print(f\"Total texts: {len(texts)}\")\n",
    "print(f\"Total IDs: {len(text_ids)}\")\n",
    "print(f\"Sample metadata: {metadatas[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation: Preparing Chunks for ChromaDB\n",
    "\n",
    "This code prepares document chunks for storage in **ChromaDB**, a vector database.\n",
    "\n",
    "### 1. Extract Text Content\n",
    "```python\n",
    "texts = [c.page_content for c in chunks]\n",
    "```\n",
    "- Creates a list of just the text content from each chunk\n",
    "- `chunks` are LangChain `Document` objects (from the EPUB loader)\n",
    "- `page_content` is the actual text of each chunk\n",
    "\n",
    "### 2. Generate Unique IDs\n",
    "```python\n",
    "text_ids = [str(uuid4())[:8] for _ in range(len(texts))]\n",
    "```\n",
    "- Creates a unique 8-character ID for each chunk\n",
    "- `uuid4()` generates a random UUID (e.g., `\"a1b2c3d4-e5f6-...\"`)\n",
    "- `[:8]` takes just the first 8 characters for brevity\n",
    "- ChromaDB requires unique IDs to identify each document\n",
    "\n",
    "### 3. Create Metadata for Source Attribution\n",
    "```python\n",
    "metadatas = [\n",
    "    {\n",
    "        \"chunk_id\": i,                                          # Position in the list (0, 1, 2...)\n",
    "        \"source\": chunks[i].metadata.get(\"source\", \"unknown\"),  # Original file/source\n",
    "        \"text_preview\": texts[i][:100] + \"...\"                  # First 100 chars preview\n",
    "    }\n",
    "    for i in range(len(texts))\n",
    "]\n",
    "```\n",
    "- Creates a metadata dictionary for each chunk containing:\n",
    "  - **`chunk_id`**: The index number of the chunk\n",
    "  - **`source`**: Where the chunk came from (e.g., the EPUB filename)\n",
    "  - **`text_preview`**: A short preview of the text (useful for debugging/display)\n",
    "\n",
    "### Why This Matters\n",
    "This metadata enables **source attribution** (Improvement #8) — when the RAG system answers a question, it can show *which chunks* the answer came from, making the system more transparent and verifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: carol_improved\n",
      "Created collection: carol_improved\n"
     ]
    }
   ],
   "source": [
    "# IMPROVEMENT #4: Create PERSISTENT Chroma client\n",
    "col_name = 'carol_improved'\n",
    "\n",
    "ch_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Drop existing collection if it exists\n",
    "try:\n",
    "    ch_client.delete_collection(col_name)\n",
    "    print(f\"Deleted existing collection: {col_name}\")\n",
    "except:\n",
    "    print(f\"No existing collection to delete\")\n",
    "\n",
    "# Create collection with metadata\n",
    "carol_col = ch_client.create_collection(\n",
    "    name=col_name,\n",
    "    embedding_function=chroma_embed_func,\n",
    "    metadata={\"description\": \"A Christmas Carol with improved chunking and metadata\"}\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation: Creating Persistent ChromaDB Collection\n",
    "\n",
    "This code sets up a **persistent vector database** using ChromaDB.\n",
    "\n",
    "### 1. Define Collection Name\n",
    "```python\n",
    "col_name = 'carol_improved'\n",
    "```\n",
    "- Names the collection for easy reference throughout the notebook\n",
    "\n",
    "### 2. Create Persistent Client\n",
    "```python\n",
    "ch_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "```\n",
    "- Creates a ChromaDB client that **saves data to disk** at `./chroma_db`\n",
    "- Unlike `chromadb.Client()` (ephemeral), this persists across sessions\n",
    "- **Benefit**: No need to re-embed documents when you restart the notebook\n",
    "\n",
    "### 3. Delete Existing Collection (if exists)\n",
    "```python\n",
    "try:\n",
    "    ch_client.delete_collection(col_name)\n",
    "except:\n",
    "    print(f\"No existing collection to delete\")\n",
    "```\n",
    "- Removes any previous version of the collection\n",
    "- Ensures a clean slate for fresh embeddings\n",
    "- Uses try/except to handle the case where collection doesn't exist yet\n",
    "\n",
    "### 4. Create New Collection\n",
    "```python\n",
    "carol_col = ch_client.create_collection(\n",
    "    name=col_name,\n",
    "    embedding_function=chroma_embed_func,\n",
    "    metadata={\"description\": \"...\"}\n",
    ")\n",
    "```\n",
    "- **`name`**: Unique identifier for this collection\n",
    "- **`embedding_function`**: The BGE model — ChromaDB will automatically embed any text added to this collection\n",
    "- **`metadata`**: Optional description for documentation\n",
    "\n",
    "### Why Persistent Storage Matters\n",
    "- **Development**: Restart kernel without re-embedding (saves 5-10 minutes)\n",
    "- **Production**: Database survives application restarts\n",
    "- **Sharing**: Can copy the `chroma_db` folder to share with others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'carol_col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Insert documents with metadata\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcarol_col\u001b[49m.add(\n\u001b[32m      3\u001b[39m     documents=texts,\n\u001b[32m      4\u001b[39m     ids=text_ids,\n\u001b[32m      5\u001b[39m     metadatas=metadatas  \u001b[38;5;66;03m# IMPROVEMENT #8: Store metadata for source attribution\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCollection contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcarol_col.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'carol_col' is not defined"
     ]
    }
   ],
   "source": [
    "# Insert documents with metadata\n",
    "carol_col.add(\n",
    "    documents=texts,\n",
    "    ids=text_ids,\n",
    "    metadatas=metadatas  # IMPROVEMENT #8: Store metadata for source attribution\n",
    ")\n",
    "\n",
    "print(f\"Collection contains {carol_col.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What happened to Marley?\n",
      "\n",
      "Top 5 results:\n",
      "\n",
      "1. ID: e6eb8a19, Distance: 0.2571\n",
      "   Preview: Their faithful Friend and Servant,\n",
      "\n",
      "C. D.\n",
      "\n",
      "December, 1843.\n",
      "\n",
      "Stave I\n",
      "\n",
      "Marley’s Ghost\n",
      "\n",
      "Marley was dead, to begin with. There is no doubt whatever about ...\n",
      "\n",
      "2. ID: 6bca2ca7, Distance: 0.3132\n",
      "   Preview: Scrooge had often heard it said that Marley had no bowels, but he had never believed it until now.\n",
      "\n",
      "No, nor did he believe it even now. Though he look...\n",
      "\n",
      "3. ID: 43eb3f5b, Distance: 0.3197\n",
      "   Preview: “It’s humbug still!” said Scrooge. “I won’t believe it.”\n",
      "\n",
      "His colour changed, though, when, without a pause, it came on through the heavy door and pas...\n",
      "\n",
      "4. ID: e2b05587, Distance: 0.3263\n",
      "   Preview: Mind! I don’t mean to say that I know of my own knowledge, what there is particularly dead about a doornail. I might have been inclined, myself, to re...\n",
      "\n",
      "5. ID: 3686ea7c, Distance: 0.3268\n",
      "   Preview: Standard Ebooks is a volunteer-driven project that produces ebook editions of public domain literature using modern typography, technology, and editor...\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"What happened to Marley?\"\n",
    "\n",
    "results = carol_col.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop 5 results:\")\n",
    "for i, (doc_id, distance, doc) in enumerate(zip(\n",
    "    results['ids'][0], \n",
    "    results['distances'][0], \n",
    "    results['documents'][0]\n",
    ")):\n",
    "    print(f\"\\n{i+1}. ID: {doc_id}, Distance: {distance:.4f}\")\n",
    "    print(f\"   Preview: {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement #6: Use Larger Model (FLAN-T5-Large)\n",
    "\n",
    "**Changed from:**\n",
    "- google/flan-t5-base (250M parameters)\n",
    "\n",
    "**Changed to:**\n",
    "- google/flan-t5-large (780M parameters)\n",
    "\n",
    "**Benefits:**\n",
    "- Better reasoning capabilities\n",
    "- More accurate answers\n",
    "- Better instruction following\n",
    "- Can handle longer contexts\n",
    "\n",
    "**Note:** If this is too slow/large, you can also try:\n",
    "- google/flan-t5-xl (3B parameters) for even better quality\n",
    "- Or keep flan-t5-base if speed is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d6c1a5cf1e48f2a236e8d0a94d73f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bde6e43eaf4e1495557cae7405ac50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c5d50d381f41e18e392da7c7a7f766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39c13325a904c578be6a2037e3bee46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474afca905c9420cb1cd1fcae4d33d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec278d46f40f483bb522fc974e549de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5006d76865144e7b7555b7e494f8eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: google/flan-t5-large\n",
      "Model parameters: 783M\n"
     ]
    }
   ],
   "source": [
    "# IMPROVEMENT #6: Load larger model\n",
    "model_name = \"google/flan-t5-base\"  # Was: flan-t5-base\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement #7: Add Reranking with Cross-Encoder\n",
    "\n",
    "**What is reranking?**\n",
    "- Initial retrieval uses bi-encoder (fast but less accurate)\n",
    "- Reranking uses cross-encoder (slower but more accurate)\n",
    "- Cross-encoder sees query + document together, not separately\n",
    "- Better at determining relevance\n",
    "\n",
    "**Process:**\n",
    "1. Retrieve top-K chunks (e.g., 10) with bi-encoder\n",
    "2. Rerank with cross-encoder\n",
    "3. Use top-N reranked chunks (e.g., 5) for answer generation\n",
    "\n",
    "**Model used:** `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "- Fast and effective\n",
    "- Trained on MS MARCO passage ranking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a5db67a3e94ec5ac47cf928196a876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7caf7d31c64139a0018b778bca2bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158c279624fa44fca5b3569d32a6d6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc836b3bf4848258074ccee223c40f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6460ab50af474ffd9b6df73195607c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb996338a5041349c682a8ae3acae03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
     ]
    }
   ],
   "source": [
    "# IMPROVEMENT #7: Load cross-encoder for reranking\n",
    "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "\n",
    "print(f\"Loaded reranker: {reranker_model_name}\")\n",
    "\n",
    "def rerank_results(query, documents, top_k=5):\n",
    "    \"\"\"\n",
    "    Rerank documents using cross-encoder.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: List of document texts\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (score, doc_index) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    # Create query-document pairs\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = reranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Get scores\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1)\n",
    "    \n",
    "    # Sort by score\n",
    "    scored_docs = [(score.item(), i) for i, score in enumerate(scores)]\n",
    "    scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    return scored_docs[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with All Improvements\n",
    "\n",
    "### Changes from Original:\n",
    "\n",
    "1. **No reformulation step** - Search directly with question\n",
    "2. **Retrieve 10 chunks** - More context candidates\n",
    "3. **Rerank to top 5** - Better relevance selection\n",
    "4. **Improved prompt** - Instructions for unknown cases\n",
    "5. **Source attribution** - Return chunk IDs and previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_improved(question, retrieve_k=10, use_k=5, show_sources=True):\n",
    "    \"\"\"\n",
    "    Improved RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        retrieve_k: Number of chunks to retrieve initially (default: 10)\n",
    "        use_k: Number of chunks to use after reranking (default: 5)\n",
    "        show_sources: Whether to show source chunks (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'answer', 'sources', and 'confidence'\n",
    "    \"\"\"\n",
    "    \n",
    "    # IMPROVEMENT #1: Skip reformulation, search with original question\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # IMPROVEMENT #2: Retrieve more chunks (10 instead of 3)\n",
    "    print(f\"Retrieving top {retrieve_k} chunks...\")\n",
    "    results = carol_col.query(\n",
    "        query_texts=[question],\n",
    "        n_results=retrieve_k,\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    documents = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "    doc_ids = results['ids'][0]\n",
    "    initial_distances = results['distances'][0]\n",
    "    \n",
    "    # IMPROVEMENT #7: Rerank with cross-encoder\n",
    "    print(f\"Reranking with cross-encoder...\")\n",
    "    reranked = rerank_results(question, documents, top_k=use_k)\n",
    "    \n",
    "    # Get top reranked documents\n",
    "    top_docs = [documents[idx] for score, idx in reranked]\n",
    "    top_metadatas = [metadatas[idx] for score, idx in reranked]\n",
    "    top_ids = [doc_ids[idx] for score, idx in reranked]\n",
    "    top_scores = [score for score, idx in reranked]\n",
    "    \n",
    "    # Combine context from top reranked chunks\n",
    "    context = \"\\n\\n---\\n\\n\".join(top_docs)\n",
    "    \n",
    "    # IMPROVEMENT #5: Better prompt with instructions for unknown cases\n",
    "    question_prompt = f\"\"\"Answer the question based on the context below. \n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- Only answer if the information is clearly stated in the context\n",
    "- If the answer is not in the context, respond with: \"I don't have enough information to answer this question.\"\n",
    "- Be concise and specific\n",
    "- Use information directly from the context\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    print(f\"Generating answer with {model_name}...\\n\")\n",
    "    enc_prompt = tokenizer(\n",
    "        question_prompt, \n",
    "        return_tensors='pt',\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    enc_answer = model.generate(\n",
    "        enc_prompt.input_ids,\n",
    "        max_length=100,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(enc_answer[0], skip_special_tokens=True)\n",
    "    \n",
    "    # IMPROVEMENT #8: Show sources with chunk IDs\n",
    "    sources = []\n",
    "    if show_sources:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ANSWER: {answer}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nSOURCES (Top {use_k} chunks after reranking):\\n\")\n",
    "        \n",
    "        for i, (doc_id, metadata, score, doc) in enumerate(zip(\n",
    "            top_ids, top_metadatas, top_scores, top_docs\n",
    "        )):\n",
    "            source_info = {\n",
    "                'chunk_id': metadata['chunk_id'],\n",
    "                'doc_id': doc_id,\n",
    "                'rerank_score': score,\n",
    "                'text_preview': doc[:200] + \"...\"\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "            \n",
    "            print(f\"{i+1}. Chunk #{metadata['chunk_id']} (ID: {doc_id})\")\n",
    "            print(f\"   Rerank Score: {score:.4f}\")\n",
    "            print(f\"   Preview: {doc[:150]}...\")\n",
    "            print()\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'num_chunks_retrieved': retrieve_k,\n",
    "        'num_chunks_used': use_k\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Improved System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the name of Bob Cratchit's youngest son who is ill?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Be concise and specific\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #235 (ID: 850e1a95)\n",
      "   Rerank Score: -1.8312\n",
      "   Preview: “Never, father!” cried they all.\n",
      "\n",
      "“And I know,” said Bob, “I know, my dears, that when we recollect how patient and how mild he was; although he was a...\n",
      "\n",
      "2. Chunk #230 (ID: 9e7c7abd)\n",
      "   Rerank Score: -2.0635\n",
      "   Preview: She hurried out to meet him; and little Bob in his comforter﻿—he had need of it, poor fellow﻿—came in. His tea was ready for him on the hob, and they ...\n",
      "\n",
      "3. Chunk #236 (ID: 72d1b038)\n",
      "   Rerank Score: -2.4814\n",
      "   Preview: “No, never, father!” they all cried again.\n",
      "\n",
      "“I am very happy,” said little Bob, “I am very happy!”\n",
      "\n",
      "Mrs. Cratchit kissed him, his daughters kissed him...\n",
      "\n",
      "4. Chunk #142 (ID: e54e0bf8)\n",
      "   Rerank Score: -2.7188\n",
      "   Preview: “We’d a deal of work to finish up last night,” replied the girl, “and had to clear away this morning, mother!”\n",
      "\n",
      "“Well! never mind so long as you are c...\n",
      "\n",
      "5. Chunk #144 (ID: baae210f)\n",
      "   Rerank Score: -3.2993\n",
      "   Preview: Martha didn’t like to see him disappointed, if it were only in joke; so she came out prematurely from behind the closet door, and ran into his arms, w...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test question 1: Simple factual\n",
    "result = answer_question_improved(\n",
    "    \"What is the name of Bob Cratchit's youngest son who is ill?\",\n",
    "    retrieve_k=10,\n",
    "    use_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was Scrooge's deceased business partner?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: I don't have enough information to answer the question.\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #18 (ID: 54a8ebd7)\n",
      "   Rerank Score: 3.1445\n",
      "   Preview: This lunatic, in letting Scrooge’s nephew out, had let two other people in. They were portly gentlemen, pleasant to behold, and now stood, with their ...\n",
      "\n",
      "2. Chunk #3 (ID: e2b05587)\n",
      "   Rerank Score: 3.0446\n",
      "   Preview: Mind! I don’t mean to say that I know of my own knowledge, what there is particularly dead about a doornail. I might have been inclined, myself, to re...\n",
      "\n",
      "3. Chunk #19 (ID: f002351e)\n",
      "   Rerank Score: 2.9361\n",
      "   Preview: “Mr. Marley has been dead these seven years,” Scrooge replied. “He died seven years ago, this very night.”\n",
      "\n",
      "“We have no doubt his liberality is well r...\n",
      "\n",
      "4. Chunk #198 (ID: 299ed78d)\n",
      "   Rerank Score: -0.0877\n",
      "   Preview: The Spirit stopped beside one little knot of business men. Observing that the hand was pointed to them, Scrooge advanced to listen to their talk.\n",
      "\n",
      "“No...\n",
      "\n",
      "5. Chunk #202 (ID: cf7b65e9)\n",
      "   Rerank Score: -0.3041\n",
      "   Preview: “No, no. Something else to think of. Good morning!”\n",
      "\n",
      "Not another word. That was their meeting, their conversation, and their parting.\n",
      "\n",
      "Scrooge was at ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test question 2: Requires inference\n",
    "result = answer_question_improved(\n",
    "    \"Who was Scrooge's deceased business partner?\",\n",
    "    retrieve_k=10,\n",
    "    use_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was Scrooge engaged to in his youth, and why did she leave him?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Be concise and specific\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 7 chunks after reranking):\n",
      "\n",
      "1. Chunk #75 (ID: 753bc4dd)\n",
      "   Rerank Score: -0.1868\n",
      "   Preview: “These are but shadows of the things that have been,” said the Ghost. “They have no consciousness of us.”\n",
      "\n",
      "The jocund travellers came on; and as they ...\n",
      "\n",
      "2. Chunk #97 (ID: dd0fb5a4)\n",
      "   Rerank Score: -0.2189\n",
      "   Preview: When the clock struck eleven, this domestic ball broke up. Mr. and Mrs. Fezziwig took their stations, one on either side the door, and, shaking hands ...\n",
      "\n",
      "3. Chunk #112 (ID: 2e96149c)\n",
      "   Rerank Score: -0.2488\n",
      "   Preview: And now Scrooge looked on more attentively than ever, when the master of the house, having his daughter leaning fondly on him, sat down with her and h...\n",
      "\n",
      "4. Chunk #15 (ID: 46f949e5)\n",
      "   Rerank Score: -0.4558\n",
      "   Preview: “But why?” cried Scrooge’s nephew. “Why?”\n",
      "\n",
      "“Why did you get married?” said Scrooge.\n",
      "\n",
      "“Because I fell in love.”\n",
      "\n",
      "“Because you fell in love!” growled Sc...\n",
      "\n",
      "5. Chunk #88 (ID: c7225ed6)\n",
      "   Rerank Score: -0.8509\n",
      "   Preview: “Always a delicate creature, whom a breath might have withered,” said the Ghost. “But she had a large heart!”\n",
      "\n",
      "“So she had,” cried Scrooge. “You’re ri...\n",
      "\n",
      "6. Chunk #100 (ID: 45deecb0)\n",
      "   Rerank Score: -1.0315\n",
      "   Preview: “My time grows short,” observed the Spirit. “Quick!”\n",
      "\n",
      "This was not addressed to Scrooge, or to anyone whom he could see, but it produced an immediate ...\n",
      "\n",
      "7. Chunk #187 (ID: 1e59f144)\n",
      "   Rerank Score: -1.2426\n",
      "   Preview: Much they saw, and far they went, and many homes they visited, but always with a happy end. The Spirit stood beside sickbeds, and they were cheerful; ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test question 3: Multiple parts\n",
    "result = answer_question_improved(\n",
    "    \"Who was Scrooge engaged to in his youth, and why did she leave him?\",\n",
    "    retrieve_k=10,\n",
    "    use_k=7  # Use more chunks for complex questions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Scrooge's favorite color?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: I don't have enough information to answer the question.\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #124 (ID: dcfa51bf)\n",
      "   Rerank Score: -0.5076\n",
      "   Preview: Scrooge entered timidly, and hung his head before this Spirit. He was not the dogged Scrooge he had been; and though the Spirit’s eyes were clear and ...\n",
      "\n",
      "2. Chunk #39 (ID: 43eb3f5b)\n",
      "   Rerank Score: -1.0077\n",
      "   Preview: “It’s humbug still!” said Scrooge. “I won’t believe it.”\n",
      "\n",
      "His colour changed, though, when, without a pause, it came on through the heavy door and pas...\n",
      "\n",
      "3. Chunk #126 (ID: ec5f8721)\n",
      "   Rerank Score: -2.9501\n",
      "   Preview: “A tremendous family to provide for,” muttered Scrooge.\n",
      "\n",
      "The Ghost of Christmas Present rose.\n",
      "\n",
      "“Spirit,” said Scrooge submissively, “conduct me where ...\n",
      "\n",
      "4. Chunk #249 (ID: 992848c9)\n",
      "   Rerank Score: -3.7638\n",
      "   Preview: Running to the window, he opened it, and put out his head. No fog, no mist; clear, bright, jovial, stirring, cold; cold, piping for the blood to dance...\n",
      "\n",
      "5. Chunk #170 (ID: e8ea7190)\n",
      "   Rerank Score: -4.1418\n",
      "   Preview: “Ha, ha! Ha, ha, ha, ha!”\n",
      "\n",
      "“He said that Christmas was a humbug, as I live!” cried Scrooge’s nephew. “He believed it, too!”\n",
      "\n",
      "“More shame for him, Fred...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test question 4: Testing \"unknown\" handling\n",
    "result = answer_question_improved(\n",
    "    \"What is Scrooge's favorite color?\",  # Not in the book\n",
    "    retrieve_k=10,\n",
    "    use_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all questions...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 1/7\n",
      "================================================================================\n",
      "Question: What is the name of Scrooge's underpaid clerk?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Only answer if the information is clearly stated in the context.\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #9 (ID: 1692abf5)\n",
      "   Rerank Score: 0.7962\n",
      "   Preview: The door of Scrooge’s counting house was open, that he might keep his eye upon his clerk, who in a dismal little cell beyond, a sort of tank, was copy...\n",
      "\n",
      "2. Chunk #28 (ID: 03f5311b)\n",
      "   Rerank Score: 0.2324\n",
      "   Preview: The clerk smiled faintly.\n",
      "\n",
      "“And yet,” said Scrooge, “you don’t think me ill used when I pay a day’s wages for no work.”\n",
      "\n",
      "The clerk observed that it wa...\n",
      "\n",
      "3. Chunk #16 (ID: ab95cb00)\n",
      "   Rerank Score: -2.8719\n",
      "   Preview: “Good afternoon,” said Scrooge.\n",
      "\n",
      "“I want nothing from you; I ask nothing of you; why cannot we be friends?”\n",
      "\n",
      "“Good afternoon!” said Scrooge.\n",
      "\n",
      "“I am so...\n",
      "\n",
      "4. Chunk #258 (ID: 2d3fc6f3)\n",
      "   Rerank Score: -3.1397\n",
      "   Preview: “Mr. Scrooge?”\n",
      "\n",
      "“Yes,” said Scrooge. “That is my name, and I fear it may not be pleasant to you. Allow me to ask your pardon. And will you have the go...\n",
      "\n",
      "5. Chunk #171 (ID: 6400d747)\n",
      "   Rerank Score: -4.4370\n",
      "   Preview: “He’s a comical old fellow,” said Scrooge’s nephew, “that’s the truth; and not so pleasant as he might be. However, his offences carry their own punis...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 2/7\n",
      "================================================================================\n",
      "Question: Who was Scrooge's deceased business partner?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: I don't have enough information to answer the question.\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #18 (ID: 54a8ebd7)\n",
      "   Rerank Score: 3.1445\n",
      "   Preview: This lunatic, in letting Scrooge’s nephew out, had let two other people in. They were portly gentlemen, pleasant to behold, and now stood, with their ...\n",
      "\n",
      "2. Chunk #3 (ID: e2b05587)\n",
      "   Rerank Score: 3.0446\n",
      "   Preview: Mind! I don’t mean to say that I know of my own knowledge, what there is particularly dead about a doornail. I might have been inclined, myself, to re...\n",
      "\n",
      "3. Chunk #19 (ID: f002351e)\n",
      "   Rerank Score: 2.9361\n",
      "   Preview: “Mr. Marley has been dead these seven years,” Scrooge replied. “He died seven years ago, this very night.”\n",
      "\n",
      "“We have no doubt his liberality is well r...\n",
      "\n",
      "4. Chunk #198 (ID: 299ed78d)\n",
      "   Rerank Score: -0.0877\n",
      "   Preview: The Spirit stopped beside one little knot of business men. Observing that the hand was pointed to them, Scrooge advanced to listen to their talk.\n",
      "\n",
      "“No...\n",
      "\n",
      "5. Chunk #202 (ID: cf7b65e9)\n",
      "   Rerank Score: -0.3041\n",
      "   Preview: “No, no. Something else to think of. Good morning!”\n",
      "\n",
      "Not another word. That was their meeting, their conversation, and their parting.\n",
      "\n",
      "Scrooge was at ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 3/7\n",
      "================================================================================\n",
      "Question: Who was Scrooge engaged to in his youth, and why did she leave him?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Be concise and specific\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #75 (ID: 753bc4dd)\n",
      "   Rerank Score: -0.1868\n",
      "   Preview: “These are but shadows of the things that have been,” said the Ghost. “They have no consciousness of us.”\n",
      "\n",
      "The jocund travellers came on; and as they ...\n",
      "\n",
      "2. Chunk #97 (ID: dd0fb5a4)\n",
      "   Rerank Score: -0.2189\n",
      "   Preview: When the clock struck eleven, this domestic ball broke up. Mr. and Mrs. Fezziwig took their stations, one on either side the door, and, shaking hands ...\n",
      "\n",
      "3. Chunk #112 (ID: 2e96149c)\n",
      "   Rerank Score: -0.2488\n",
      "   Preview: And now Scrooge looked on more attentively than ever, when the master of the house, having his daughter leaning fondly on him, sat down with her and h...\n",
      "\n",
      "4. Chunk #15 (ID: 46f949e5)\n",
      "   Rerank Score: -0.4558\n",
      "   Preview: “But why?” cried Scrooge’s nephew. “Why?”\n",
      "\n",
      "“Why did you get married?” said Scrooge.\n",
      "\n",
      "“Because I fell in love.”\n",
      "\n",
      "“Because you fell in love!” growled Sc...\n",
      "\n",
      "5. Chunk #88 (ID: c7225ed6)\n",
      "   Rerank Score: -0.8509\n",
      "   Preview: “Always a delicate creature, whom a breath might have withered,” said the Ghost. “But she had a large heart!”\n",
      "\n",
      "“So she had,” cried Scrooge. “You’re ri...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 4/7\n",
      "================================================================================\n",
      "Question: What is the name of Bob Cratchit's youngest son who is ill?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Be concise and specific\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #235 (ID: 850e1a95)\n",
      "   Rerank Score: -1.8312\n",
      "   Preview: “Never, father!” cried they all.\n",
      "\n",
      "“And I know,” said Bob, “I know, my dears, that when we recollect how patient and how mild he was; although he was a...\n",
      "\n",
      "2. Chunk #230 (ID: 9e7c7abd)\n",
      "   Rerank Score: -2.0635\n",
      "   Preview: She hurried out to meet him; and little Bob in his comforter﻿—he had need of it, poor fellow﻿—came in. His tea was ready for him on the hob, and they ...\n",
      "\n",
      "3. Chunk #236 (ID: 72d1b038)\n",
      "   Rerank Score: -2.4814\n",
      "   Preview: “No, never, father!” they all cried again.\n",
      "\n",
      "“I am very happy,” said little Bob, “I am very happy!”\n",
      "\n",
      "Mrs. Cratchit kissed him, his daughters kissed him...\n",
      "\n",
      "4. Chunk #142 (ID: e54e0bf8)\n",
      "   Rerank Score: -2.7188\n",
      "   Preview: “We’d a deal of work to finish up last night,” replied the girl, “and had to clear away this morning, mother!”\n",
      "\n",
      "“Well! never mind so long as you are c...\n",
      "\n",
      "5. Chunk #144 (ID: baae210f)\n",
      "   Rerank Score: -3.2993\n",
      "   Preview: Martha didn’t like to see him disappointed, if it were only in joke; so she came out prematurely from behind the closet door, and ran into his arms, w...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 5/7\n",
      "================================================================================\n",
      "Question: What does Scrooge see written on the gravestone that frightens him into changing his ways?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Use information directly from the context\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #240 (ID: 00761e6e)\n",
      "   Rerank Score: 3.2744\n",
      "   Preview: “Before I draw nearer to that stone to which you point,” said Scrooge, “answer me one question. Are these the shadows of the things that Will be, or a...\n",
      "\n",
      "2. Chunk #241 (ID: 307f1c67)\n",
      "   Rerank Score: 1.9229\n",
      "   Preview: The Spirit was immovable as ever.\n",
      "\n",
      "Scrooge crept towards it, trembling as he went; and, following the finger, read upon the stone of the neglected gra...\n",
      "\n",
      "3. Chunk #195 (ID: b186d8ec)\n",
      "   Rerank Score: 0.7823\n",
      "   Preview: The upper portion of the garment was contracted for an instant in its folds, as if the Spirit had inclined its head. That was the only answer he recei...\n",
      "\n",
      "4. Chunk #45 (ID: 670f9471)\n",
      "   Rerank Score: 0.4947\n",
      "   Preview: At this the spirit raised a frightful cry, and shook its chain with such a dismal and appalling noise, that Scrooge held on tight to his chair, to sav...\n",
      "\n",
      "5. Chunk #220 (ID: 94089630)\n",
      "   Rerank Score: 0.3802\n",
      "   Preview: Scrooge glanced towards the Phantom. Its steady hand was pointed to the head. The cover was so carelessly adjusted that the slightest raising of it, t...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 6/7\n",
      "================================================================================\n",
      "Question: What is Scrooge's response when his nephew Fred invites him to Christmas dinner at the beginning of the story?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: I don't have enough information to answer the question.\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #185 (ID: 8ba31ad8)\n",
      "   Rerank Score: 4.4074\n",
      "   Preview: “He has given us plenty of merriment, I am sure,” said Fred, “and it would be ungrateful not to drink his health. Here is a glass of mulled wine ready...\n",
      "\n",
      "2. Chunk #10 (ID: 6ff77000)\n",
      "   Rerank Score: 3.5314\n",
      "   Preview: “A merry Christmas, uncle! God save you!” cried a cheerful voice. It was the voice of Scrooge’s nephew, who came upon him so quickly that this was the...\n",
      "\n",
      "3. Chunk #12 (ID: bfab18f5)\n",
      "   Rerank Score: 3.1639\n",
      "   Preview: “Uncle!” pleaded the nephew.\n",
      "\n",
      "“Nephew!” returned the uncle sternly, “keep Christmas in your own way, and let me keep it in mine.”\n",
      "\n",
      "“Keep it!” repeated...\n",
      "\n",
      "4. Chunk #263 (ID: f4b25739)\n",
      "   Rerank Score: 2.7773\n",
      "   Preview: “Why, bless my soul!” cried Fred, “who’s that?”\n",
      "\n",
      "“It’s I. Your uncle Scrooge. I have come to dinner. Will you let me in, Fred?”\n",
      "\n",
      "Let him in! It is a m...\n",
      "\n",
      "5. Chunk #11 (ID: 90b21ccd)\n",
      "   Rerank Score: 2.5579\n",
      "   Preview: “Come, then,” returned the nephew gaily. “What right have you to be dismal? What reason have you to be morose? You’re rich enough.”\n",
      "\n",
      "Scrooge, having n...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 7/7\n",
      "================================================================================\n",
      "Question: What specific, generous act does Scrooge perform for the Cratchit family on Christmas morning?\n",
      "\n",
      "Retrieving top 10 chunks...\n",
      "Reranking with cross-encoder...\n",
      "Generating answer with google/flan-t5-large...\n",
      "\n",
      "================================================================================\n",
      "ANSWER: Be concise and specific\n",
      "================================================================================\n",
      "\n",
      "SOURCES (Top 5 chunks after reranking):\n",
      "\n",
      "1. Chunk #138 (ID: 3890fa06)\n",
      "   Rerank Score: 2.1946\n",
      "   Preview: And perhaps it was the pleasure the good Spirit had in showing off this power of his, or else it was his own kind, generous, hearty nature, and his sy...\n",
      "\n",
      "2. Chunk #126 (ID: ec5f8721)\n",
      "   Rerank Score: 1.5385\n",
      "   Preview: “A tremendous family to provide for,” muttered Scrooge.\n",
      "\n",
      "The Ghost of Christmas Present rose.\n",
      "\n",
      "“Spirit,” said Scrooge submissively, “conduct me where ...\n",
      "\n",
      "3. Chunk #157 (ID: d0065fec)\n",
      "   Rerank Score: 0.4144\n",
      "   Preview: “It should be Christmas Day, I am sure,” said she, “on which one drinks the health of such an odious, stingy, hard, unfeeling man as Mr. Scrooge. You ...\n",
      "\n",
      "4. Chunk #156 (ID: db522fa5)\n",
      "   Rerank Score: 0.1400\n",
      "   Preview: Scrooge bent before the Ghost’s rebuke, and, trembling, cast his eyes upon the ground. But he raised them speedily on hearing his own name.\n",
      "\n",
      "“Mr. Scro...\n",
      "\n",
      "5. Chunk #236 (ID: 72d1b038)\n",
      "   Rerank Score: -0.2730\n",
      "   Preview: “No, never, father!” they all cried again.\n",
      "\n",
      "“I am very happy,” said little Bob, “I am very happy!”\n",
      "\n",
      "Mrs. Cratchit kissed him, his daughters kissed him...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load questions from file if available\n",
    "questions = [\n",
    "    \"What is the name of Scrooge's underpaid clerk?\",\n",
    "    \"Who was Scrooge's deceased business partner?\",\n",
    "    \"Who was Scrooge engaged to in his youth, and why did she leave him?\",\n",
    "    \"What is the name of Bob Cratchit's youngest son who is ill?\",\n",
    "    \"What does Scrooge see written on the gravestone that frightens him into changing his ways?\",\n",
    "    \"What is Scrooge's response when his nephew Fred invites him to Christmas dinner at the beginning of the story?\",\n",
    "    \"What specific, generous act does Scrooge perform for the Cratchit family on Christmas morning?\"\n",
    "]\n",
    "\n",
    "print(\"Processing all questions...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}/{len(questions)}\")\n",
    "    print(\"=\"*80)\n",
    "    result = answer_question_improved(q, retrieve_k=10, use_k=5, show_sources=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Aspect | Original | Improved | Impact |\n",
    "|--------|----------|----------|--------|\n",
    "| Chunking overlap | 50 chars (5%) | 512 chars (50%) | Better context preservation |\n",
    "| Storage | Ephemeral (lost on restart) | Persistent (saved to disk) | No re-embedding needed |\n",
    "| Retrieval | 3 chunks | 10 chunks → reranked to 5 | More comprehensive context |\n",
    "| Question processing | 2-step reformulation | Direct search | Simpler, faster, more accurate |\n",
    "| Model size | FLAN-T5-base (250M) | FLAN-T5-large (780M) | Better reasoning |\n",
    "| Reranking | None | Cross-encoder | Better relevance scoring |\n",
    "| Prompt quality | Basic | Instructive with unknown handling | Better answers |\n",
    "| Source attribution | None | Chunk IDs + metadata | Verifiable answers |\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "The improved system should:\n",
    "- Give more accurate answers\n",
    "- Handle complex multi-part questions better\n",
    "- Correctly identify when information is not available\n",
    "- Provide source citations for verification\n",
    "- Run faster after first initialization (persistent storage)\n",
    "- Be more production-ready\n",
    "\n",
    "### Further Improvements (Optional)\n",
    "\n",
    "If you want to go even further:\n",
    "1. Use a modern LLM (Claude, GPT-4, Llama) instead of FLAN-T5\n",
    "2. Add query expansion (generate multiple query variations)\n",
    "3. Implement hybrid search (keyword + semantic)\n",
    "4. Add semantic chunking (split on topics, not characters)\n",
    "5. Use a better embedding model (e.g., OpenAI ada-002, Cohere embed-v3)\n",
    "6. Add metadata filtering (by chapter, character, etc.)\n",
    "7. Implement multi-hop reasoning for complex questions\n",
    "8. Add answer validation/verification step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
