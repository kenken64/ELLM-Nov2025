{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Question and Answers\n",
    "In this workshop, you will learning how to write prompts and feed them into LLMs. You\n",
    "will also be learning how to use different prompt techniques to improve the response\n",
    "from the LLM.\n",
    "\n",
    "## Loading and Explorng the Dataset\n",
    "The workshop will be using [`facebook/ExploreToM`](https://huggingface.co/datasets/facebook/ExploreToM) dataset from [HuggingFace](https://huggingface.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the following libraries: datasets\n",
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name = \"facebook/ExploreToM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and explore the dataset\n",
    "ds = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (13309, 18)}\n",
      "dict_keys(['train'])\n",
      "{'story_structure': Value('string'), 'infilled_story': Value('string'), 'question': Value('string'), 'expected_answer': Value('string'), 'qprop=params': Value('string'), 'qprop=nth_order': Value('int64'), 'qprop=non_unique_mental_state': Value('bool'), 'sprop=is_false_belief_story_1st': Value('bool'), 'sprop=is_false_belief_story_1st_and_2nd': Value('bool'), 'sprop=story_accuracy_1st_raw': Value('float64'), 'sprop=story_accuracy_1st_infilled': Value('float64'), 'sprop=global_idx': Value('int64'), 'param=story_type': Value('string'), 'param=num_stories_total': Value('int64'), 'param=max_sentences': Value('int64'), 'param=num_people': Value('int64'), 'param=num_moves': Value('int64'), 'param=num_rooms': Value('int64')}\n",
      "key = story_structure, value = Kaylee entered the hotel lobby. Kaylee moved the silver letter opener to the wooden desk drawer, which is also located in the hotel lobby. While this action was happening, Liam witnessed this action in secret (and only this action). Kaylee left the hotel lobby. Liam entered the hotel lobby. Kaylee entered the hotel lobby. Liam moved the silver letter opener to the leather briefcase, which is also located in the hotel lobby.\n",
      "key = infilled_story, value = The sun shone through the large glass doors of the hotel lobby, illuminating the marble floor and casting a warm glow over the comfortable seating areas. Soft music filled the air, mingling with the gentle hum of conversation and the occasional chime of the elevators in the bustling hotel. As Kaylee entered the hotel lobby her eyes quickly scanned the space, taking in every detail to ensure everything was in order for the upcoming event. Kaylee's task of tidying the lobby extended to the small, silver item, which she carefully stowed away in the desk drawer, and Liam, observing from across the room, felt his interest in the object grow, his mind racing with questions about its significance and purpose. With her tasks in the lobby complete, Kaylee stepped out into the fresh air, the sounds of the bustling hotel lobby fading into the background as the glass doors slid shut behind her. Liam moved across the lobby floor, his footsteps silent on the marble as he walked towards the empty reception desk where the mystery item was now hidden. Moments later, Kaylee stepped back through the glass doors of the hotel, joining Liam in the lobby once again. Liam's interest in the silver item led him to reposition it, now resting snugly within the hotel lobby's nearby leather briefcase. Meanwhile, Kaylee began to run a final check on the lobby, seemingly unconcerned by the briefcase's new contents.\n",
      "key = question, value = In which container is the silver letter opener now?\n",
      "key = expected_answer, value = leather briefcase\n",
      "key = qprop=params, value = (None, 'silver letter opener', 'ground_truth-container_location')\n",
      "key = qprop=nth_order, value = -1\n",
      "key = qprop=non_unique_mental_state, value = True\n",
      "key = sprop=is_false_belief_story_1st, value = False\n",
      "key = sprop=is_false_belief_story_1st_and_2nd, value = False\n",
      "key = sprop=story_accuracy_1st_raw, value = 0.0\n",
      "key = sprop=story_accuracy_1st_infilled, value = 0.0\n",
      "key = sprop=global_idx, value = 0\n",
      "key = param=story_type, value = tomi+object-state+asymmetric\n",
      "key = param=num_stories_total, value = 10\n",
      "key = param=max_sentences, value = 15\n",
      "key = param=num_people, value = 2\n",
      "key = param=num_moves, value = 2\n",
      "key = param=num_rooms, value = 1\n"
     ]
    }
   ],
   "source": [
    "# TODO: number of rows in the dataset\n",
    "print(ds.shape)\n",
    "\n",
    "# TODO: Keys in the dataset\n",
    "print(ds.keys())\n",
    "\n",
    "# TODO: Feature names\n",
    "print(ds['train'].features)\n",
    "\n",
    "# TODO: Display a single row\n",
    "idx = 1\n",
    "for k, v in ds['train'][idx].items():\n",
    "   print(f'key = {k}, value = {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pipeline`\n",
    "[`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) is an easy to use API to perform inferencing. It provides a wrapper for task-specific pipelines and abstracts most of the complexity by allowing you to focus on the model and the task. \n",
    "\n",
    "You can use `pipeline` to perform summarisation, image classification, audio generation, etc. You can find an exhaustive list of `pipeline` task [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# TODO: Summarise the text with the pipeline's default model\n",
    "\n",
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predition:  {'score': 0.683438777923584, 'start': 286, 'end': 301, 'answer': 'leather satchel'}\n",
      "Actual:  wooden chest\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "question = ds['train'][idx]['question']\n",
    "story = ds['train'][idx]['story_structure']\n",
    "#story = ds['train'][idx]['infilled_story']\n",
    "answer = ds['train'][idx]['expected_answer']\n",
    "\n",
    "predict_answer = qna(question=question, context=story)\n",
    "\n",
    "print('Predition: ', predict_answer)\n",
    "print('Actual: ', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference - Question and Answer\n",
    "In this section, we will look at what `pipeline` does under the hood to perform its inference. This will give us a better understanding of the major steps involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT base cased distilled SQuAD\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. More details [here](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   157, 14640,   188, 24537,  1136,  1294,   170,  3395,  1107,\n",
      "          1103,  1176,  1757,  1104,  1103,  1769,  1713,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode text\n",
    "text = 'hello world'\n",
    "text = 'Thou shalt not make a machine in the likeness of the human mind.'\n",
    "\n",
    "# return tokens as Pytorch Tensors\n",
    "enc_text = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Thou shalt not make a machine in the likeness of the human mind. [SEP]\n"
     ]
    }
   ],
   "source": [
    "dec_text = tokenizer.decode(enc_text.input_ids[0])\n",
    "print(dec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(101) [CLS]\n",
      "tensor(157) T\n",
      "tensor(14640) ##hou\n",
      "tensor(188) s\n",
      "tensor(24537) ##halt\n",
      "tensor(1136) not\n",
      "tensor(1294) make\n",
      "tensor(170) a\n",
      "tensor(3395) machine\n",
      "tensor(1107) in\n",
      "tensor(1103) the\n",
      "tensor(1176) like\n",
      "tensor(1757) ##ness\n",
      "tensor(1104) of\n",
      "tensor(1103) the\n",
      "tensor(1769) human\n",
      "tensor(1713) mind\n",
      "tensor(119) .\n",
      "tensor(102) [SEP]\n"
     ]
    }
   ],
   "source": [
    "for tok in enc_text.input_ids[0]:\n",
    "   dec_tok = tokenizer.decode(tok)\n",
    "   print(tok, dec_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2562,  1602, 15430, 24752,  1116,  1602,  1892,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   189,  1110,   170,  3062, 23578,  8646,   117,  1115,   170,\n",
      "          1423,  1299,  1107,  6224,  1104,   170,  1363,  8977,   117,  1538,\n",
      "          1129,  1107,  1328,  1104,   170,  1676,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encoding multiple texts\n",
    "texts = [\n",
    "   \"Big black bug bleeds black blood\",\n",
    "   \"t is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife\"\n",
    "]\n",
    "\n",
    "enc_texts = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "\n",
    "print(enc_texts.input_ids)\n",
    "print(enc_texts.attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decode text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "Create and instance of the Large Language Model (LLM). We will then create a simple\n",
    "prompt, tokenize the prompt and feed the tokenized prompt to the LLM. The response\n",
    "from the LLM will be decoded to human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load libraries\n",
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load question answer model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1327,  1674, 10206,  1341,  1164, 23065,   112,   188,  6369,\n",
      "          1113,  3782,  6213, 10700,   136,   113,  3520,  1164,  1122,   120,\n",
      "          1674,  1136,  1221,  1164,  1122,   114,   102,  1109,  3258,  8656,\n",
      "          1104,  5101,  4204, 15888,  1103, 18652,  5769,  1104,  1103,  3782,\n",
      "          4745,   117,  9616,   170, 21088,  6814,  1166, 10389,  1104, 12960,\n",
      "          1116,  1105, 14312,   119,   138,  7859,  5974,  1104,  3618, 13433,\n",
      "          1105,  6656, 20049, 20619,  1194,  1103,  1586,   117, 11241, 10134,\n",
      "          1114,  1103,  6531,  3807,  1104,  7053,  1105,  1390,   117,  1112,\n",
      "          1103,  1480,   112,   188, 24039,  1125,  1198,  4972,  1106,  8362,\n",
      "         10787,   119,  8521,  5002, 25649,  1154,  1103,  1514,  9459,   117,\n",
      "          1103,  3807,  1104,  1103,  3782,  1796, 17429, 17032,  1118,  1103,\n",
      "         11246,  2928,   119,  8521,  4144,  1763,  1103,  5134, 11246, 23841,\n",
      "          1116,   117, 17862,  1114,  1103, 21088,  3515,  1796,   117,  1147,\n",
      "          3999,  3170,  6532, 22604, 17429, 25752,  1181, 20746, 17814,  3782,\n",
      "          2758,  1468,   119,  1109,  5092,  9404,  1866,  4432,  1481,  1103,\n",
      "         12960,  1116,   117,  1157,  1442,  2494,   172, 22362,  1158,  1501,\n",
      "          1106,  5890, 10206,   119,  1249,  1131,  2242,   117,  1103,  3807,\n",
      "          1104,  7053,  1105,  1390,  2580, 10884,  1200,   117,  2125,  1118,\n",
      "          1103,  1538,  1183,  5974,  1104,  7905,  5508,  1105,  1103,  7859,\n",
      "           187,  8954,  1513,  1104,  8113,   119,  1249, 10206,  3876, 19587,\n",
      "          1103,  5092,  9404,   112,   188,  8792,   117,  1103, 12084, 16225,\n",
      "          1276,   170,  1207,  8137,  1282,  1120,  1103,  3248,  1104,  1103,\n",
      "          1385,  4122,  2229,   117,  1157,  2525,  1183,  1257,   176, 22761,\n",
      "          1158, 20525,  1107,  1103, 12563,  1609,   119,  8521,  1108,  6843,\n",
      "          1118,  1103,  1514,  9459,   112,   188, 12563,  1193,  4941,  4604,\n",
      "           117,  4016,  3525,  1103,   171,  8586, 25710,  1361,  6814,  1106,\n",
      "         21689,  1147,  5172,   117,  1196,  1231,  5521, 27884,  1154,  1103,\n",
      "          1480,   112,   188, 24039,  1796,   117,  3170,  6532, 22604,  1208,\n",
      "         27104,  2343,  1306,  8709,  1154,  1103, 17814, 13484,   119,  1249,\n",
      "         10206,  3175,  1103, 12084, 16225,  1106,  1103,  1514,  9459,   117,\n",
      "          1103, 21088, 13287,  2083,  1105,  1461,  1104,  1103,  3782,   112,\n",
      "           188, 23150, 17429, 14100,  1149,  1103,  1839,  1104,  1123, 10139,\n",
      "           117,  2128,  1103,  5092,  9404,   118,  1105,  1157,  4122,  2229,\n",
      "           118,  4432,  1481,   119,  1109,  8059,  1104,  1123,  1490, 11241,\n",
      "          8384,  1114,  1103,  1839,  1104,  7053,  1105,  1390,   117,  4004,\n",
      "         10206,   112,   188,  3578,  1113,  1103,  3782,   112,   188,  6213,\n",
      "          5250, 14291,  1116,  1149,  1506,  1103,  4745,   117,   170, 20049,\n",
      "         16364,  5126,  1104, 14222,  1286,  1107,  1123,  5314,   119, 10206,\n",
      "         22546,   170,  3802,  1106, 23065,   117,  1150, 18942,  1120,  1103,\n",
      "          2652,  1104,  1103,  3515,   117,  1123,  1257, 16348,  4016,  2135,\n",
      "          1117,  1112,  1131, 22546,   117,   112,  1124,   112,   188,  1171,\n",
      "          1107,  1103,  1514,  9459,   117,   112,  1105, 23065,   112,   188,\n",
      "          1257, 16945,   117, 25568,  1103,  3318,  3802,  1196,  1119,  1286,\n",
      "          1106,  1231, 14572,  1471,  1107,  1103,  3876,  1104,  1103,  1514,\n",
      "          9459,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode context and question\n",
    "idx = 100\n",
    "question = ds['train'][idx]['question']\n",
    "story = ds['train'][idx]['story_structure']\n",
    "story = ds['train'][idx]['infilled_story']\n",
    "answer = ds['train'][idx]['expected_answer']\n",
    "\n",
    "enc_question = tokenizer(question, return_tensors='pt')\n",
    "enc_story = tokenizer(story, return_tensors='pt')\n",
    "\n",
    "enc_input = tokenizer(question, story, return_tensors='pt', padding=True)\n",
    "print(enc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] In which container was the pocket watch at the beginning? [SEP] The bustling theater was a hive of activity on this chilly autumn evening, its worn wooden floors and faded velvet curtains a testament to years of countless performances. The dimly lit green room, tucked away backstage, was a cozy refuge from the chaos, its plush armchairs and ornate wooden chest offering a warm respite for those who needed it. Dylan slipped away from the crowd, disappearing behind a tattered curtain as he made his way to a more secluded space. The soft glow of a floor lamp in the green room enveloped him, providing a sense of tranquility amidst the pre - show chaos. Dylan carefully placed the pocket watch in the ornate wooden chest, hidden from view, and Clayton caught a glimpse of this sneaky maneuver from his secret vantage point. Clayton ' s eyes narrowed as he pushed open the creaky door and stepped into the green room, the sudden movement making the softly lit space seem almost anticipatory. In a swift motion, Dylan delved into the leather satchel, repositioning its contents to accommodate the pocket watch, which now nestled among its weathered confines in the green room. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(enc_input.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(enc_input.input_ids, enc_input.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -3.3331,  -7.0589,  -7.4497,  -6.9617,  -8.8258,  -8.2602,  -7.9495,\n",
      "          -8.8655,  -8.0126,  -8.1991,  -7.5855,  -5.1998,  -7.5699,  -3.9040,\n",
      "          -6.8398,  -6.2395,  -4.4169,  -7.9310,  -7.0676,  -3.1802,  -5.1120,\n",
      "          -4.8768,  -4.3839,  -6.6323,  -3.5747,   2.9140,   3.6676,   1.6920,\n",
      "          -7.0250,  -7.1047,  -8.1779,  -8.5559,  -7.2667,  -6.7811,  -5.9012,\n",
      "          -4.7313,  -7.8112,  -7.6731,  -6.8141,  -7.7848,  -8.1972,  -9.7429,\n",
      "          -9.0243,  -8.9493,  -4.6271,  -8.0457,  -7.6556,  -9.0675,  -7.7875,\n",
      "          -6.2783,  -8.4216, -10.1118,  -8.9245,  -7.9942,  -8.5808,  -9.3803,\n",
      "          -7.4695,  -4.4636,  -7.2955,  -6.1989,  -4.6764,  -7.6352,  -5.7930,\n",
      "          -1.5447,  -3.4093,  -1.6110,  -2.4669,  -5.1233,  -1.5189,   5.8467,\n",
      "           7.0104,   4.0358,  -0.2083,  -5.4350,  -5.8350,  -6.3616,  -7.2949,\n",
      "          -5.6393,  -4.6958,  -3.9456,  -3.6151,  -6.5309,  -6.0961,  -7.5699]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[ -1.9622,  -8.2763,  -7.5890,  -6.5472,  -8.7597,  -9.7610,  -9.3724,\n",
      "          -7.5789,  -8.8846,  -8.7485,  -6.7426,  -5.8158,  -8.3998,  -4.9317,\n",
      "          -8.2817,  -8.4747,  -5.9577,  -3.9517,  -5.1423,  -4.9345,  -7.4131,\n",
      "          -8.4095,  -7.1271,  -4.5941,  -7.4888,  -4.9183,  -0.7089,   4.5613,\n",
      "          -0.3357,  -6.1758,  -8.7349,  -7.9865,  -7.2835,  -9.2066,  -8.4482,\n",
      "          -6.5968,  -2.9670,  -3.5340,  -9.2686,  -9.2126,  -8.1439,  -9.7927,\n",
      "          -6.3220,  -7.5358,  -5.8543,  -8.3605,  -8.4105,  -6.6186, -10.9694,\n",
      "          -4.6378,  -7.4798, -10.3499,  -9.5500,  -8.9741,  -6.4614,  -6.0422,\n",
      "          -5.5987,  -5.7387,  -8.1485,  -8.4915,  -6.3941,  -3.7750,  -4.4321,\n",
      "          -4.2159,  -5.8713,  -6.8723,  -6.2941,  -2.8290,  -5.8003,  -2.9803,\n",
      "           2.2458,  -1.3603,   8.3353,   2.4547,  -3.8401,  -6.2117,  -6.1655,\n",
      "          -4.8322,  -7.3281,  -7.1829,  -5.7291,  -0.4179,   0.3740,  -8.3998]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10206, 22546,   170,  3802])\n",
      "Isabella mouthed a message\n",
      "does not know about it\n"
     ]
    }
   ],
   "source": [
    "# TODO: Tokenize the inputs\n",
    "start_ans = torch.argmax(result.start_logits)\n",
    "end_ans = torch.argmax(result.end_logits) + 1\n",
    "enc_answer = enc_input.input_ids[0][start_ans: end_ans]\n",
    "\n",
    "print(enc_answer)\n",
    "predict_answer = tokenizer.decode(enc_answer)\n",
    "print(predict_answer)\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure minimum and maximum token length in the answer\n",
    "import torch\n",
    "def ensure_size(input_ids, answer, min_length = 2, max_length = 5):\n",
    "   ans_start = torch.argmax(answer['start_logits'])\n",
    "   ans_end = torch.argmax(answer['end_logits']) + 1\n",
    "   ans_length = ans_end - ans_start\n",
    "   if ans_length < min_length:\n",
    "      ans_end = min(ans_start + min_length, len(input_ids[0]))\n",
    "   elif ans_length > max_length:\n",
    "      ans_end = ans_start + max_length\n",
    "   ans_start = max(0, ans_start)\n",
    "   ans_end = min(len(input_ids[0]), ans_end)\n",
    "   return (ans_start, ans_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Return a minimum of 5 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try this your self\n",
    "\n",
    "context = \"\"\"\n",
    "Dickens wrote A Christmas Carol during a period when the British were exploring and re-evaluating past Christmas traditions, \n",
    "including carols, and newer customs such as cards and Christmas trees. He was influenced by the experiences of his own youth and \n",
    "by the Christmas stories of other authors, including Washington Irving and Douglas Jerrold. Dickens had written three Christmas \n",
    "stories prior to the novella, and was inspired following a visit to the Field Lane Ragged School, one of several establishments for \n",
    "London's street children. The treatment of the poor and the ability of a selfish man to redeem himself by transforming into a more \n",
    "sympathetic character are the key themes of the story. There is discussion among academics as to whether this is a fully secular \n",
    "story or a Christian allegory.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How many stories has Dickens wrote?\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellm-2025-nov17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
